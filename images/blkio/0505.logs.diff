diff --git a/Makefile b/Makefile
index 69fa5c0310d8..3c1257682830 100644
--- a/Makefile
+++ b/Makefile
@@ -664,7 +664,7 @@ else
 ifdef CONFIG_PROFILE_ALL_BRANCHES
 KBUILD_CFLAGS	+= -O2 $(call cc-disable-warning,maybe-uninitialized,)
 else
-KBUILD_CFLAGS   += -O2
+KBUILD_CFLAGS   += -O1
 endif
 endif
 
diff --git a/block/bio.c b/block/bio.c
index 0093bed81c0e..a5066cf723c3 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1995,7 +1995,7 @@ int bio_associate_blkcg_from_page(struct bio *bio, struct page *page)
 int bio_associate_blkcg(struct bio *bio, struct cgroup_subsys_state *blkcg_css)
 {
 	if (unlikely(bio->bi_css))
-		return -EBUSY;
+		return -EBUSY; //CX____ may cost time?
 	css_get(blkcg_css);
 	bio->bi_css = blkcg_css;
 	return 0;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index c630e02836a8..cb8a3c0f54b9 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -821,6 +821,7 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	int key_len, part, ret;
 	char *body;
 
+	printk(KERN_ERR "blkg_conf_prep %s", input);
 	if (sscanf(input, "%u:%u%n", &major, &minor, &key_len) != 2)
 		return -EINVAL;
 
@@ -830,8 +831,11 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	body = skip_spaces(body);
 
 	disk = get_gendisk(MKDEV(major, minor), &part);
-	if (!disk)
+	if (!disk) {
+		printk(KERN_ERR "blkg_conf_prep null diks");
 		return -ENODEV;
+	}
+	printk(KERN_ERR "blkg_conf_prep part=%d", part);
 	if (part) {
 		ret = -ENODEV;
 		goto fail;
@@ -1217,6 +1221,8 @@ int blkcg_init_queue(struct request_queue *q)
 	bool preloaded;
 	int ret;
 
+    printk(KERN_ERR "%s[%d] ---blkfg init queue---\n", __FUNCTION__, __LINE__);
+    trace_printk("%s[%d] ---blkfg init queue---\n", __FUNCTION__, __LINE__);
 	new_blkg = blkg_alloc(&blkcg_root, q, GFP_KERNEL);
 	if (!new_blkg)
 		return -ENOMEM;
@@ -1619,6 +1625,7 @@ static void blkcg_scale_delay(struct blkcg_gq *blkg, u64 now)
 {
 	u64 old = atomic64_read(&blkg->delay_start);
 
+    trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/*
 	 * We only want to scale down every second.  The idea here is that we
 	 * want to delay people for min(delay_nsec, NSEC_PER_SEC) in a certain
@@ -1675,6 +1682,7 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	u64 delay_nsec = 0;
 	int tok;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	while (blkg->parent) {
 		if (atomic_read(&blkg->use_delay)) {
 			blkcg_scale_delay(blkg, now);
@@ -1693,6 +1701,8 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	 * delay, and we want userspace to be able to do _something_ so cap the
 	 * delays at 1 second.  If there's 10's of seconds worth of delay then
 	 * the tasks will be delayed for 1 second for every syscall.
+	 * 如果我们耽搁了太久，就不要睡到永远。
+交换或元数据IO可以累积10秒的延迟，我们希望用户空间能够做些什么，所以将延迟限制在1秒。如果有10秒的延迟，那么每个系统调用的任务将延迟1秒。
 	 */
 	delay_nsec = min_t(u64, delay_nsec, 250 * NSEC_PER_MSEC);
 
@@ -1702,6 +1712,9 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	 * to do a psi_memstall_enter/leave if memdelay is set.
 	 */
 
+	trace_printk("%s[%d] --- delay_nsec=%lly\n", __FUNCTION__, __LINE__, delay_nsec);
+	printk(KERN_ERR "%s[%d] --- delay_nsec=%lly\n", __FUNCTION__, __LINE__, delay_nsec);
+
 	exp = ktime_add_ns(now, delay_nsec);
 	tok = io_schedule_prepare();
 	do {
@@ -1721,6 +1734,7 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
  * anything.  This should only ever be called by the resume code, it's not meant
  * to be called by people willy-nilly as it will actually do the work to
  * throttle the task if it is setup for throttling.
+ * blkcg_maybe_throttle_current-如果当前任务已标记，则对其进行限制。仅当我们被标记为set_notify_resume（）时才调用此函数。显然，我们可以因为blkcg throttling以外的原因设置notify_resume（），所以我们检查current->throttling_queue是否已设置，如果未设置，则不会执行任何操作。这应该只由恢复代码调用，而不是由人们随意调用，因为如果设置为限制任务，它将实际执行限制任务的工作。
  */
 void blkcg_maybe_throttle_current(void)
 {
@@ -1733,6 +1747,7 @@ void blkcg_maybe_throttle_current(void)
 	if (!q)
 		return;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	current->throttle_queue = NULL;
 	current->use_memdelay = false;
 
@@ -1788,8 +1803,10 @@ void blkcg_schedule_throttle(struct request_queue *q, bool use_memdelay)
 	if (!blk_get_queue(q))
 		return;
 
-	if (current->throttle_queue)
-		blk_put_queue(current->throttle_queue);
+	if (current->throttle_queue) {
+		trace_printk("%s[%d] --- current->throttle_queue.id=%d\n", __FUNCTION__, __LINE__, current->throttle_queue->id);
+		blk_put_queue(current->throttle_queue); //减小引用？
+	}
 	current->throttle_queue = q;
 	if (use_memdelay)
 		current->use_memdelay = use_memdelay;
@@ -1809,6 +1826,7 @@ void blkcg_add_delay(struct blkcg_gq *blkg, u64 now, u64 delta)
 {
 	blkcg_scale_delay(blkg, now);
 	atomic64_add(delta, &blkg->delay_nsec);
+	trace_printk("CX____ %s[%d] add %llu (ns), to %llu (ns)\n", __FUNCTION__, __LINE__, delta, blkg->delay_nsec);
 }
 EXPORT_SYMBOL_GPL(blkcg_add_delay);
 
diff --git a/block/blk-core.c b/block/blk-core.c
index cff0a60ee200..0f4294f6e9b4 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -145,7 +145,9 @@ EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_clear);
 
 static void blk_clear_congested(struct request_list *rl, int sync)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	clear_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/*
@@ -159,7 +161,9 @@ static void blk_clear_congested(struct request_list *rl, int sync)
 
 static void blk_set_congested(struct request_list *rl, int sync)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	set_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/* see blk_clear_congested() */
@@ -172,19 +176,23 @@ void blk_queue_congestion_threshold(struct request_queue *q)
 {
 	int nr;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	nr = q->nr_requests - (q->nr_requests / 8) + 1;
 	if (nr > q->nr_requests)
 		nr = q->nr_requests;
 	q->nr_congestion_on = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_on=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_on);
 
 	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
 	if (nr < 1)
 		nr = 1;
 	q->nr_congestion_off = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_off=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_off);
 }
 
 void blk_rq_init(struct request_queue *q, struct request *rq)
 {
+    //printk(KERN_ERR "%s[%d]: ---init---", __FUNCTION__, __LINE__);
 	memset(rq, 0, sizeof(*rq));
 
 	INIT_LIST_HEAD(&rq->queuelist);
@@ -1000,6 +1008,8 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id,
 	struct request_queue *q;
 	int ret;
 
+    printk(KERN_ERR "%s[%d] ---alloc request queue---\n", __FUNCTION__, __LINE__);
+    trace_printk("%s[%d] ---alloc request queue---\n", __FUNCTION__, __LINE__);
 	q = kmem_cache_alloc_node(blk_requestq_cachep,
 				gfp_mask | __GFP_ZERO, node_id);
 	if (!q)
@@ -1354,6 +1364,7 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	const bool is_sync = op_is_sync(op);
 	int may_queue;
 	req_flags_t rq_flags = RQF_ALLOCED;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 
@@ -1465,6 +1476,8 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	if (ioc_batching(q, ioc))
 		ioc->nr_batch_requests--;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_getrq(q, bio, op);
 	return rq;
 
@@ -1532,6 +1545,7 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 	DEFINE_WAIT(wait);
 	struct request_list *rl;
 	struct request *rq;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
@@ -1556,6 +1570,8 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 	prepare_to_wait_exclusive(&rl->wait[is_sync], &wait,
 				  TASK_UNINTERRUPTIBLE);
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_sleeprq(q, bio, op);
 
 	spin_unlock_irq(q->queue_lock);
@@ -1645,11 +1661,15 @@ EXPORT_SYMBOL(blk_get_request);
  */
 void blk_requeue_request(struct request_queue *q, struct request *rq)
 {
+	struct task_struct *tsk = current;
+
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
 	blk_delete_timer(rq);
 	blk_clear_rq_complete(rq);
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_rq_requeue(q, rq);
 	rq_qos_requeue(q, rq);
 
@@ -1798,11 +1818,14 @@ EXPORT_SYMBOL(blk_put_request);
 bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 			    struct bio *bio)
 {
+	struct task_struct *tsk = current;
 	const int ff = bio->bi_opf & REQ_FAILFAST_MASK;
 
 	if (!ll_back_merge_fn(q, req, bio))
 		return false;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_bio_backmerge(q, req, bio);
 
 	if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
@@ -1820,11 +1843,14 @@ bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 			     struct bio *bio)
 {
+	struct task_struct *tsk = current;
 	const int ff = bio->bi_opf & REQ_FAILFAST_MASK;
 
 	if (!ll_front_merge_fn(q, req, bio))
 		return false;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_bio_frontmerge(q, req, bio);
 
 	if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
@@ -1992,6 +2018,7 @@ static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)
 	int where = ELEVATOR_INSERT_SORT;
 	struct request *req, *free;
 	unsigned int request_count = 0;
+	struct task_struct *tsk = current;
 
 	/*
 	 * low level driver can indicate that it wants pages above a
@@ -2090,6 +2117,8 @@ static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)
 		 * @request_count may become stale because of schedule
 		 * out, so check plug list again.
 		 */
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		if (!request_count || list_empty(&plug->list))
 			trace_block_plug(q);
 		else {
@@ -2213,6 +2242,7 @@ static inline int blk_partition_remap(struct bio *bio)
 {
 	struct hd_struct *p;
 	int ret = -EIO;
+	struct task_struct *tsk = current;
 
 	rcu_read_lock();
 	p = __disk_get_part(bio->bi_disk, bio->bi_partno);
@@ -2231,6 +2261,8 @@ static inline int blk_partition_remap(struct bio *bio)
 		if (bio_check_eod(bio, part_nr_sects_read(p)))
 			goto out;
 		bio->bi_iter.bi_sector += p->start_sect;
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		trace_block_bio_remap(bio->bi_disk->queue, bio, part_devt(p),
 				      bio->bi_iter.bi_sector - p->start_sect);
 	}
@@ -2248,6 +2280,7 @@ generic_make_request_checks(struct bio *bio)
 	int nr_sectors = bio_sectors(bio);
 	blk_status_t status = BLK_STS_IOERR;
 	char b[BDEVNAME_SIZE];
+	struct task_struct *tsk = current;
 
 	might_sleep();
 
@@ -2332,6 +2365,8 @@ generic_make_request_checks(struct bio *bio)
 		return false;
 
 	if (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		trace_block_bio_queue(q, bio);
 		/* Now that enqueuing has been traced, we need to trace
 		 * completion as well.
@@ -2869,6 +2904,7 @@ struct request *blk_peek_request(struct request_queue *q)
 {
 	struct request *rq;
 	int ret;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
@@ -2889,6 +2925,8 @@ struct request *blk_peek_request(struct request_queue *q)
 			 * not be passed by new incoming requests
 			 */
 			rq->rq_flags |= RQF_STARTED;
+			if (tsk)
+				trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 			trace_block_rq_issue(q, rq);
 		}
 
@@ -3077,7 +3115,10 @@ bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
 	int total_bytes;
+	struct task_struct *tsk = current;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_rq_complete(req, blk_status_to_errno(error), nr_bytes);
 
 	if (!req->bio)
@@ -3629,8 +3670,11 @@ static void queue_unplugged(struct request_queue *q, unsigned int depth,
 			    bool from_schedule)
 	__releases(q->queue_lock)
 {
+	struct task_struct *tsk = current;
 	lockdep_assert_held(q->queue_lock);
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_unplug(q, depth, !from_schedule);
 
 	if (from_schedule)
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index 19923f8a029d..de77428702b4 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -77,6 +77,8 @@
 
 #define DEFAULT_SCALE_COOKIE 1000000U
 
+static int g_dump_stack_counter = 0;
+
 static struct blkcg_policy blkcg_policy_iolatency;
 struct iolatency_grp;
 
@@ -88,11 +90,13 @@ struct blk_iolatency {
 
 static inline struct blk_iolatency *BLKIOLATENCY(struct rq_qos *rqos)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return container_of(rqos, struct blk_iolatency, rqos);
 }
 
 static inline bool blk_iolatency_enabled(struct blk_iolatency *blkiolat)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return atomic_read(&blkiolat->enabled) > 0;
 }
 
@@ -177,11 +181,19 @@ static inline bool iolatency_may_queue(struct iolatency_grp *iolat,
 				       bool first_block)
 {
 	struct rq_wait *rqw = &iolat->rq_wait;
+    int ret = 0;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	/* waitqueue_active() returns true if the wait list is not empty */
 	if (first_block && waitqueue_active(&rqw->wait) &&
-	    rqw->wait.head.next != &wait->entry)
+	    rqw->wait.head.next != &wait->entry) {
+		trace_printk("%s[%d] --- return false\n", __FUNCTION__, __LINE__);
 		return false;
-	return rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	}
+	//return rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	ret = rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	trace_printk("%s[%d] --- max_depth=%llu\n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
+    return ret;
 }
 
 static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
@@ -193,24 +205,34 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 {
 	struct rq_wait *rqw = &iolat->rq_wait;
 	unsigned use_delay = atomic_read(&lat_to_blkg(iolat)->use_delay);
-	DEFINE_WAIT(wait);
+	DEFINE_WAIT(wait); // now we have a wait entry
 	bool first_block = true;
+    int tmp_inflight = 0;
 
-	if (use_delay)
-		blkcg_schedule_throttle(rqos->q, use_memdelay);
+	trace_printk("%s[%d] --- Entry, current latency=%llu\n", __FUNCTION__, __LINE__, iolat->min_lat_nsec);
+	if (use_delay) {
+		trace_printk("%s[%d] --- use_delay ^^^ use_memdelay=%d\n", __FUNCTION__, __LINE__, use_memdelay);
+		blkcg_schedule_throttle(rqos->q, use_memdelay); // use delay to memcg ?? check later
+	}
 
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 	/*
 	 * To avoid priority inversions we want to just take a slot if we are
 	 * issuing as root.  If we're being killed off there's no point in
 	 * delaying things, we may have been killed by OOM so throttling may
 	 * make recovery take even longer, so just let the IO's through so the
 	 * task can go away.
+	 * 为了避免优先级反转，如果我们以根用户身份发布，我们只需要占用一个插槽。如果我们被杀了，拖延时间是没有意义的，我们可能已经被OOM杀死了，所以节流可能会使恢复时间更长，所以让IO完成任务就可以了。
 	 */
 	if (issue_as_root || fatal_signal_pending(current)) {
+        tmp_inflight = atomic_read(&rqw->inflight);
 		atomic_inc(&rqw->inflight);
+        if (atomic_read(&rqw->inflight) != tmp_inflight)
+	        trace_printk("%s[%d] --- inflight chenged from %d to %d\n", __FUNCTION__, __LINE__, tmp_inflight, rqw->inflight);
 		return;
 	}
 
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 	if (iolatency_may_queue(iolat, &wait, first_block))
 		return;
 
@@ -218,12 +240,14 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 		prepare_to_wait_exclusive(&rqw->wait, &wait,
 					  TASK_UNINTERRUPTIBLE);
 
+	    trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 		if (iolatency_may_queue(iolat, &wait, first_block))
 			break;
 		first_block = false;
 
 		if (lock) {
 			spin_unlock_irq(lock);
+			trace_printk("%s[%d] --- io_schedule()\n", __FUNCTION__, __LINE__);
 			io_schedule();
 			spin_lock_irq(lock);
 		} else {
@@ -232,6 +256,8 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 	} while (1);
 
 	finish_wait(&rqw->wait, &wait);
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
+	trace_printk("%s[%d] --- Eixt, current latency=%llu\n", __FUNCTION__, __LINE__, iolat->min_lat_nsec);
 }
 
 #define SCALE_DOWN_FACTOR 2
@@ -250,6 +276,7 @@ static inline unsigned long scale_amount(unsigned long qd, bool up)
  * Each group has their own local copy of the last scale cookie they saw, so if
  * the global scale cookie goes up or down they know which way they need to go
  * based on their last knowledge of it.
+ * 我们缩小qd的速度比放大快，所以我们需要使用这个助手相应地调整缩放cookie，这样我们就不会过早地获得默认scale_cookie下的scale_cookie，并且不会太多。每个小组都有他们自己的本地副本，他们看到的最后一个scale cookie，因此如果全局scale cookie上升或下降，他们知道他们需要走哪条路基于他们最后的知识。
  */
 static void scale_cookie_change(struct blk_iolatency *blkiolat,
 				struct child_latency_info *lat_info,
@@ -261,9 +288,13 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 	unsigned long max_scale = qd << 1;
 	unsigned long diff = 0;
 
+	trace_printk("%s[%d] --- entry, old cookie=%d, up=%d\n", __FUNCTION__, __LINE__, old, up);
+	printk(KERN_ERR "%s[%d] --- entry, old cookie=%d, up=%d\n", __FUNCTION__, __LINE__, old, up);
 	if (old < DEFAULT_SCALE_COOKIE)
 		diff = DEFAULT_SCALE_COOKIE - old;
 
+	trace_printk("%s[%d] --- entry, queue_deepth=%d, diff=%d\n", __FUNCTION__, __LINE__, qd, diff);
+	printk(KERN_ERR "%s[%d] --- entry, queue_deepth=%d, diff=%d\n", __FUNCTION__, __LINE__, qd, diff);
 	if (up) {
 		if (scale + old > DEFAULT_SCALE_COOKIE)
 			atomic_set(&lat_info->scale_cookie,
@@ -278,6 +309,7 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 		 * dig out of it.  Just enough that we don't throttle/unthrottle
 		 * with jagged workloads but can still unthrottle once pressure
 		 * has sufficiently dissipated.
+         * 我们不想挖这么深的洞，要花几个小时才能挖出来。就足够了，我们不会在工作负荷参差不齐的情况下节流/松开节流阀，但一旦压力充分消散，我们仍然可以松开节流阀。
 		 */
 		if (diff > qd) {
 			if (diff < max_scale)
@@ -286,6 +318,8 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 			atomic_sub(scale, &lat_info->scale_cookie);
 		}
 	}
+	trace_printk("%s[%d] --- finish, cookie=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter);
+	printk(KERN_ERR "%s[%d] --- finish, cookie=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter);
 }
 
 /*
@@ -303,6 +337,7 @@ static void scale_change(struct iolatency_grp *iolat, bool up)
 	if (old > qd)
 		old = qd;
 
+	trace_printk("%s[%d] --- Entry, max_depth is %d , up=%d\n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth, up);
 	if (up) {
 		if (old == 1 && blkcg_unuse_delay(lat_to_blkg(iolat)))
 			return;
@@ -312,13 +347,16 @@ static void scale_change(struct iolatency_grp *iolat, bool up)
 			old += scale;
 			old = min(old, qd);
 			iolat->rq_depth.max_depth = old;
+			trace_printk("%s[%d] --- up, max_depth added %lu to %d \n", __FUNCTION__, __LINE__, scale, iolat->rq_depth.max_depth);
 			wake_up_all(&iolat->rq_wait.wait);
 		}
 	} else if (old > 1) {
 		old >>= 1;
 		changed = true;
 		iolat->rq_depth.max_depth = max(old, 1UL);
+		trace_printk("%s[%d] --- down, max_depth to %d \n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
 	}
+	trace_printk("%s[%d] --- Exit, max_depth to %d \n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
 }
 
 /* Check our parent and see if the scale cookie has changed. */
@@ -331,6 +369,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	u64 scale_lat;
 	unsigned int old;
 	int direction = 0;
+	static int count = 0;
 
 	if (lat_to_blkg(iolat)->parent == NULL)
 		return;
@@ -339,10 +378,18 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	if (!parent)
 		return;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	lat_info = &parent->child_lat;
 	cur_cookie = atomic_read(&lat_info->scale_cookie);
 	scale_lat = READ_ONCE(lat_info->scale_lat);
 
+	if (count > 300) {
+		trace_printk("%s[%d] --- scale_lat=%llu, iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, scale_lat, iolat->min_lat_nsec);
+		count = 0;
+	} else {
+		count++;
+	}
+
 	if (cur_cookie < our_cookie)
 		direction = -1;
 	else if (cur_cookie > our_cookie)
@@ -370,6 +417,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 		 */
 		samples_thresh = lat_info->nr_samples * 5;
 		samples_thresh = div64_u64(samples_thresh, 100);
+		trace_printk("%s[%d] --- samples_thresh=%llu, lat_info->nr_samples=%llu\n", __FUNCTION__, __LINE__, samples_thresh, lat_info->nr_samples);
 		if (iolat->nr_samples <= samples_thresh)
 			return;
 	}
@@ -377,6 +425,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	/* We're as low as we can go. */
 	if (iolat->rq_depth.max_depth == 1 && direction < 0) {
 		blkcg_use_delay(lat_to_blkg(iolat));
+		trace_printk("%s[%d] --- we already as low as we can go !!!  rq_depth.max_depth==1\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
@@ -385,9 +434,11 @@ static void check_scale_change(struct iolatency_grp *iolat)
 		blkcg_clear_delay(lat_to_blkg(iolat));
 		iolat->rq_depth.max_depth = UINT_MAX;
 		wake_up_all(&iolat->rq_wait.wait);
+		trace_printk("%s[%d] --- We're back to the default cookie, unthrottle all the things.\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
+	trace_printk("%s[%d] --- scale\n", __FUNCTION__, __LINE__);
 	scale_change(iolat, direction > 0);
 }
 
@@ -400,12 +451,21 @@ static void blkcg_iolatency_throttle(struct rq_qos *rqos, struct bio *bio,
 	struct request_queue *q = rqos->q;
 	bool issue_as_root = bio_issue_as_root_blkg(bio);
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	if (!blk_iolatency_enabled(blkiolat))
 		return;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	blkcg = bio_blkcg(bio);
-	bio_associate_blkcg(bio, &blkcg->css);
+	bio_associate_blkcg(bio, &blkcg->css); // bio->bi_css = blkcg_css;
 	blkg = blkg_lookup(blkcg, q);
 	if (unlikely(!blkg)) {
 		if (!lock)
@@ -420,10 +480,16 @@ static void blkcg_iolatency_throttle(struct rq_qos *rqos, struct bio *bio,
 		goto out;
 
 	bio_issue_init(&bio->bi_issue, bio_sectors(bio));
-	bio_associate_blkg(bio, blkg);
+	bio_associate_blkg(bio, blkg); // bio->bi_blkg = blkg;
+	struct iolatency_grp *tmp_iolat = blkg_to_lat(blkg);
+    if (!tmp_iolat)
+	    trace_printk("%s[%d] --- bio->bi_issue=%llu\n", __FUNCTION__, __LINE__, bio->bi_issue.value);
+    else
+	    trace_printk("%s[%d] --- lat-%llu --- bio->bi_issue=%llu\n", __FUNCTION__, __LINE__, tmp_iolat->min_lat_nsec, bio->bi_issue.value);
+
 out:
 	rcu_read_unlock();
-	while (blkg && blkg->parent) {
+	while (blkg && blkg->parent) { //从下往上
 		struct iolatency_grp *iolat = blkg_to_lat(blkg);
 		if (!iolat) {
 			blkg = blkg->parent;
@@ -447,6 +513,7 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 	u64 start = bio_issue_time(issue);
 	u64 req_time;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Have to do this so we are truncated to the correct time that our
 	 * issue is truncated to.
@@ -466,11 +533,13 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 		u64 sub = iolat->min_lat_nsec;
 		if (req_time < sub)
 			blkcg_add_delay(lat_to_blkg(iolat), now, sub - req_time);
+			//trace_printk("%s[%d] --- root grp, add delay\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
 	rq_stat = get_cpu_ptr(iolat->stats);
 	blk_rq_stat_add(rq_stat, req_time);
+	trace_printk("%s[%d] --- blk_rq_stat_add=%llu\n", __FUNCTION__, __LINE__, req_time);
 	put_cpu_ptr(rq_stat);
 }
 
@@ -486,6 +555,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	unsigned long flags;
 	int cpu, exp_idx;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blk_rq_stat_init(&stat);
 	preempt_disable();
 	for_each_online_cpu(cpu) {
@@ -497,10 +567,14 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	preempt_enable();
 
 	parent = blkg_to_lat(blkg->parent);
-	if (!parent)
+	if (!parent) {
+		trace_printk("%s[%d] --- ignore root grp?\n", __FUNCTION__, __LINE__);
 		return;
+	}
 
 	lat_info = &parent->child_lat;
+	trace_printk("CX____ %s[%d]: lat_info->scale_lat=%lld\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
+	//printk(KERN_ERR "CX____ %s[%d]: lat_info->scale_lat=%lld", __FUNCTION__, __LINE__, lat_info->scale_lat);
 
 	/*
 	 * CALC_LOAD takes in a number stored in fixed point representation.
@@ -508,6 +582,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	 * are significantly larger than the FIXED_1 denominator (2048).
 	 * Therefore, rounding errors in the calculation are negligible and
 	 * can be ignored.
+     * CALC_LOAD接受以定点表示形式存储的数字。因为我们在ns中使用这个作为IO时间，所以存储的值明显大于FIXED_1分母（2048）。因此，计算中的舍入误差可以忽略不计。
 	 */
 	exp_idx = min_t(int, BLKIOLATENCY_NR_EXP_FACTORS - 1,
 			div64_u64(iolat->cur_win_nsec,
@@ -516,8 +591,10 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 
 	/* Everything is ok and we don't need to adjust the scale. */
 	if (stat.mean <= iolat->min_lat_nsec &&
-	    atomic_read(&lat_info->scale_cookie) == DEFAULT_SCALE_COOKIE)
+	    atomic_read(&lat_info->scale_cookie) == DEFAULT_SCALE_COOKIE) {
+		trace_printk("%s[%d] --- everything is ok\n", __FUNCTION__, __LINE__);
 		return;
+    }
 
 	/* Somebody beat us to the punch, just bail. */
 	spin_lock_irqsave(&lat_info->lock, flags);
@@ -525,15 +602,21 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	lat_info->nr_samples += stat.nr_samples;
 	iolat->nr_samples = stat.nr_samples;
 
+    trace_printk("%s[%d]: scale_lat=%llu\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
 	if ((lat_info->last_scale_event >= now ||
 	    now - lat_info->last_scale_event < BLKIOLATENCY_MIN_ADJUST_TIME) &&
-	    lat_info->scale_lat <= iolat->min_lat_nsec)
+	    lat_info->scale_lat <= iolat->min_lat_nsec){
+        trace_printk("%s[%d]: no need, goto out\n", __FUNCTION__, __LINE__);
 		goto out;
+	}
 
+	trace_printk("%s[%d] --- stat.mean=%llu  iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, stat.mean, iolat->min_lat_nsec);
+	printk(KERN_ERR "%s[%d] --- stat.mean=%llu  iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, stat.mean, iolat->min_lat_nsec);
 	if (stat.mean <= iolat->min_lat_nsec &&
 	    stat.nr_samples >= BLKIOLATENCY_MIN_GOOD_SAMPLES) {
 		if (lat_info->scale_grp == iolat) {
 			lat_info->last_scale_event = now;
+            trace_printk("%s[%d]: to call scale_cookie_change, up\n", __FUNCTION__, __LINE__);
 			scale_cookie_change(iolat->blkiolat, lat_info, true);
 		}
 	} else if (stat.mean > iolat->min_lat_nsec) {
@@ -559,11 +642,15 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 	bool issue_as_root = bio_issue_as_root_blkg(bio);
 	bool enabled = false;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkg = bio->bi_blkg;
 	if (!blkg)
 		return;
 
 	iolat = blkg_to_lat(bio->bi_blkg);
+	if (!issue_as_root) {
+		trace_printk("CX____ %s[%d]\n", __FUNCTION__, __LINE__);
+	}
 	if (!iolat)
 		return;
 
@@ -577,18 +664,22 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 		rqw = &iolat->rq_wait;
 
 		atomic_dec(&rqw->inflight);
+	    trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 		if (!enabled || iolat->min_lat_nsec == 0)
 			goto next;
 		iolatency_record_time(iolat, &bio->bi_issue, now,
 				      issue_as_root);
+		trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 		window_start = atomic64_read(&iolat->window_start);
 		if (now > window_start &&
 		    (now - window_start) >= iolat->cur_win_nsec) {
 			if (atomic64_cmpxchg(&iolat->window_start,
 					window_start, now) == window_start)
+				trace_printk("CX____ %s[%d] --- over window? will check latency  now-window_start=%llu\n", __FUNCTION__, __LINE__, now-window_start);
 				iolatency_check_latencies(iolat, now);
 		}
 next:
+		trace_printk("CX____ %s[%d] --- wake_up rqw->wait  !!!!!\n", __FUNCTION__, __LINE__);
 		wake_up(&rqw->wait);
 		blkg = blkg->parent;
 	}
@@ -619,6 +710,7 @@ static void blkcg_iolatency_exit(struct rq_qos *rqos)
 {
 	struct blk_iolatency *blkiolat = BLKIOLATENCY(rqos);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	del_timer_sync(&blkiolat->timer);
 	blkcg_deactivate_policy(rqos->q, &blkcg_policy_iolatency);
 	kfree(blkiolat);
@@ -638,6 +730,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	struct cgroup_subsys_state *pos_css;
 	u64 now = ktime_to_ns(ktime_get());
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	blkg_for_each_descendant_pre(blkg, pos_css,
 				     blkiolat->rqos.q->root_blkg) {
@@ -672,6 +765,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * on.
 		 */
 		if (lat_info->scale_grp == NULL) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			scale_cookie_change(iolat->blkiolat, lat_info, true);
 			goto next_lock;
 		}
@@ -682,11 +776,15 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * doing any IO currently.
 		 */
 		if (now - lat_info->last_scale_event >=
-		    ((u64)NSEC_PER_SEC * 5))
+		    ((u64)NSEC_PER_SEC * 5)) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			lat_info->scale_grp = NULL;
+		}
 next_lock:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		spin_unlock_irqrestore(&lat_info->lock, flags);
 next:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		blkg_put(blkg);
 	}
 	rcu_read_unlock();
@@ -698,6 +796,7 @@ int blk_iolatency_init(struct request_queue *q)
 	struct rq_qos *rqos;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkiolat = kzalloc(sizeof(*blkiolat), GFP_KERNEL);
 	if (!blkiolat)
 		return -ENOMEM;
@@ -727,10 +826,13 @@ static void iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 	struct blk_iolatency *blkiolat = iolat->blkiolat;
 	u64 oldval = iolat->min_lat_nsec;
 
+	trace_printk("%s[%d] --- val=%llu\n", __FUNCTION__, __LINE__, val);
+	printk(KERN_ERR "%s[%d] --- val=%llu", __FUNCTION__, __LINE__, val);
 	iolat->min_lat_nsec = val;
 	iolat->cur_win_nsec = max_t(u64, val << 4, BLKIOLATENCY_MIN_WIN_SIZE);
 	iolat->cur_win_nsec = min_t(u64, iolat->cur_win_nsec,
 				    BLKIOLATENCY_MAX_WIN_SIZE);
+	printk(KERN_ERR "%s[%d] --- cur_win_nsec=%llu", __FUNCTION__, __LINE__, iolat->cur_win_nsec);
 
 	if (!oldval && val)
 		atomic_inc(&blkiolat->enabled);
@@ -740,6 +842,7 @@ static void iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 
 static void iolatency_clear_scaling(struct blkcg_gq *blkg)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (blkg->parent) {
 		struct iolatency_grp *iolat = blkg_to_lat(blkg->parent);
 		struct child_latency_info *lat_info;
@@ -769,6 +872,7 @@ static ssize_t iolatency_set_limit(struct kernfs_open_file *of, char *buf,
 	u64 oldval;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	ret = blkg_conf_prep(blkcg, &blkcg_policy_iolatency, buf, &ctx);
 	if (ret)
 		return ret;
@@ -820,6 +924,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	const char *dname = blkg_dev_name(pd->blkg);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (!dname || !iolat->min_lat_nsec)
 		return 0;
 	seq_printf(sf, "%s target=%llu\n",
@@ -829,6 +934,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 
 static int iolatency_print_limit(struct seq_file *sf, void *v)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),
 			  iolatency_prfill_limit,
 			  &blkcg_policy_iolatency, seq_cft(sf)->private, false);
@@ -842,6 +948,7 @@ static size_t iolatency_pd_stat(struct blkg_policy_data *pd, char *buf,
 	unsigned long long avg_lat = div64_u64(iolat->lat_avg, NSEC_PER_USEC);
 	unsigned long long cur_win = div64_u64(iolat->cur_win_nsec, NSEC_PER_MSEC);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (iolat->rq_depth.max_depth == UINT_MAX)
 		return scnprintf(buf, size, " depth=max avg_lat=%llu win=%llu",
 				 avg_lat, cur_win);
@@ -855,6 +962,7 @@ static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp, int node)
 {
 	struct iolatency_grp *iolat;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	iolat = kzalloc_node(sizeof(*iolat), gfp, node);
 	if (!iolat)
 		return NULL;
@@ -876,6 +984,7 @@ static void iolatency_pd_init(struct blkg_policy_data *pd)
 	u64 now = ktime_to_ns(ktime_get());
 	int cpu;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		struct blk_rq_stat *stat;
 		stat = per_cpu_ptr(iolat->stats, cpu);
@@ -911,12 +1020,14 @@ static void iolatency_pd_offline(struct blkg_policy_data *pd)
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	struct blkcg_gq *blkg = lat_to_blkg(iolat);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	iolatency_set_min_lat_nsec(blkg, 0);
 	iolatency_clear_scaling(blkg);
 }
 
 static void iolatency_pd_free(struct blkg_policy_data *pd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	free_percpu(iolat->stats);
 	kfree(iolat);
@@ -943,11 +1054,13 @@ static struct blkcg_policy blkcg_policy_iolatency = {
 
 static int __init iolatency_init(void)
 {
+	trace_printk("CX____ registered iolatency blkcg_policy\n\n");
 	return blkcg_policy_register(&blkcg_policy_iolatency);
 }
 
 static void __exit iolatency_exit(void)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return blkcg_policy_unregister(&blkcg_policy_iolatency);
 }
 
diff --git a/block/blk-rq-qos.c b/block/blk-rq-qos.c
index 0005dfd568dd..823d0c85bc72 100644
--- a/block/blk-rq-qos.c
+++ b/block/blk-rq-qos.c
@@ -1,5 +1,7 @@
 #include "blk-rq-qos.h"
 
+static int g_dump_stack_counter = 0;
+
 /*
  * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
  * false if 'v' + 1 would be bigger than 'below'.
@@ -8,6 +10,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 {
 	unsigned int cur = atomic_read(v);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (;;) {
 		unsigned int old;
 
@@ -24,6 +27,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 
 bool rq_wait_inc_below(struct rq_wait *rq_wait, unsigned int limit)
 {
+	trace_printk("%s[%d] --- set rq_weit->inflight to %d\n", __FUNCTION__, __LINE__, limit);
 	return atomic_inc_below(&rq_wait->inflight, limit);
 }
 
@@ -31,6 +35,7 @@ void rq_qos_cleanup(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->cleanup)
 			rqos->ops->cleanup(rqos, bio);
@@ -41,6 +46,12 @@ void rq_qos_done(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done)
 			rqos->ops->done(rqos, rq);
@@ -51,6 +62,12 @@ void rq_qos_issue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->issue)
 			rqos->ops->issue(rqos, rq);
@@ -61,6 +78,12 @@ void rq_qos_requeue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->requeue)
 			rqos->ops->requeue(rqos, rq);
@@ -72,7 +95,13 @@ void rq_qos_throttle(struct request_queue *q, struct bio *bio,
 {
 	struct rq_qos *rqos;
 
-	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+
+	for(rqos = q->rq_qos; rqos; rqos = rqos->next) { // 可能同时有多个qos?
 		if (rqos->ops->throttle)
 			rqos->ops->throttle(rqos, bio, lock);
 	}
@@ -82,6 +111,15 @@ void rq_qos_track(struct request_queue *q, struct request *rq, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->track)
 			rqos->ops->track(rqos, rq, bio);
@@ -92,6 +130,12 @@ void rq_qos_done_bio(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done_bio)
 			rqos->ops->done_bio(rqos, bio);
@@ -106,6 +150,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 	unsigned int depth;
 	bool ret = false;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * For QD=1 devices, this is a special case. It's important for those
 	 * to have one request ready when one completes, so force a depth of
@@ -143,13 +188,19 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 		}
 
 		rqd->max_depth = depth;
+		trace_printk("%s[%d] ---max_depth=%d\n", __FUNCTION__, __LINE__, rqd->max_depth);
+		printk(KERN_ERR "%s[%d] ---max_depth=%d\n", __FUNCTION__, __LINE__, rqd->max_depth);
 	}
 
+    if (ret) {
+        printk(KERN_ERR "%s[%d] --- couldn't increase depth futher, rqd->max_depth=%d\n", __FUNCTION__, __LINE__, rqd->max_depth);
+    }
 	return ret;
 }
 
 void rq_depth_scale_up(struct rq_depth *rqd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Hit max in previous round, stop here
 	 */
@@ -159,6 +210,8 @@ void rq_depth_scale_up(struct rq_depth *rqd)
 	rqd->scale_step--;
 
 	rqd->scaled_max = rq_depth_calc_max_depth(rqd);
+    printk(KERN_ERR "%s[%d] --- couldn't increase depth futher, rqd->scaled_max=%d\n", __FUNCTION__, __LINE__, rqd->scaled_max);
+    printk(KERN_ERR "%s[%d] --- rqd->scale_step=%d\n", __FUNCTION__, __LINE__, rqd->scale_step);
 }
 
 /*
@@ -167,6 +220,7 @@ void rq_depth_scale_up(struct rq_depth *rqd)
  */
 void rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Stop scaling down when we've hit the limit. This also prevents
 	 * ->scale_step from going to crazy values, if the device can't
@@ -182,10 +236,12 @@ void rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 
 	rqd->scaled_max = false;
 	rq_depth_calc_max_depth(rqd);
+    printk(KERN_ERR "%s[%d] --- rqd->scale_step=%d\n", __FUNCTION__, __LINE__, rqd->scale_step);
 }
 
 void rq_qos_exit(struct request_queue *q)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (q->rq_qos) {
 		struct rq_qos *rqos = q->rq_qos;
 		q->rq_qos = rqos->next;
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 32b02efbfa66..09d346662f9c 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -74,6 +74,7 @@ static inline void rq_wait_init(struct rq_wait *rq_wait)
 
 static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rqos->next = q->rq_qos;
 	q->rq_qos = rqos;
 }
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index 01d0620a4e4a..5f8bd4dca0fb 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2148,9 +2148,12 @@ bool blk_throtl_bio(struct request_queue *q, struct blkcg_gq *blkg,
 
 	WARN_ON_ONCE(!rcu_read_lock_held());
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/* see throtl_charge_bio() */
-	if (bio_flagged(bio, BIO_THROTTLED) || !tg->has_rules[rw])
+	if (bio_flagged(bio, BIO_THROTTLED) || !tg->has_rules[rw]) {
 		goto out;
+	} else 
+		trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 
 	spin_lock_irq(q->queue_lock);
 
@@ -2213,7 +2216,8 @@ bool blk_throtl_bio(struct request_queue *q, struct blkcg_gq *blkg,
 	}
 
 	/* out-of-limit, queue to @tg */
-	throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
+	//throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
+	trace_printk("[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d\n",
 		   rw == READ ? 'R' : 'W',
 		   tg->bytes_disp[rw], bio->bi_iter.bi_size,
 		   tg_bps_limit(tg, rw),
@@ -2508,6 +2512,7 @@ static int __init throtl_init(void)
 	if (!kthrotld_workqueue)
 		panic("Failed to create kthrotld\n");
 
+	printk(KERN_ERR "CX____ registered throtl blkcg_policy\n");
 	return blkcg_policy_register(&blkcg_policy_throtl);
 }
 
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index 8ac93fcbaa2e..c47ddc79ba71 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -123,6 +123,7 @@ static void rwb_wake_all(struct rq_wb *rwb)
 	}
 }
 
+/* 将不需要wait或者不需要继续wait的队列唤醒 */
 static void wbt_rqw_done(struct rq_wb *rwb, struct rq_wait *rqw,
 			 enum wbt_flags wb_acct)
 {
@@ -157,7 +158,7 @@ static void wbt_rqw_done(struct rq_wb *rwb, struct rq_wait *rqw,
 	if (inflight && inflight >= limit)
 		return;
 
-	if (wq_has_sleeper(&rqw->wait)) {
+	if (wq_has_sleeper(&rqw->wait)) { // eturns true if the wait list is not empty
 		int diff = limit - inflight;
 
 		if (!inflight || diff >= rwb->wb_background / 2)
@@ -220,6 +221,7 @@ static u64 rwb_sync_issue_lat(struct rq_wb *rwb)
 		return 0;
 
 	now = ktime_to_ns(ktime_get());
+	trace_printk("%s[%d] --- issue_lat=%llu\n", __FUNCTION__, __LINE__, now-issue);
 	return now - issue;
 }
 
@@ -235,6 +237,16 @@ static int latency_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
 	struct backing_dev_info *bdi = rwb->rqos.q->backing_dev_info;
 	struct rq_depth *rqd = &rwb->rq_depth;
 	u64 thislat;
+    static int dump_cnt = 0;
+
+    if (dump_cnt > 30) {
+        printk(KERN_ERR "%s[%d] ---\n", __FUNCTION__, __LINE__);
+	    //dump_stack();
+        dump_cnt = 0;
+    } else {
+        dump_cnt++;
+    }
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 
 	/*
 	 * If our stored sync issue exceeds the window size, or it
@@ -270,6 +282,7 @@ static int latency_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
 
 	/*
 	 * If the 'min' latency exceeds our target, step down.
+     * 如果“最小”延迟超过了我们的目标，请step down
 	 */
 	if (stat[READ].min > rwb->min_lat_nsec) {
 		trace_wbt_lat(bdi, stat[READ].min);
@@ -303,10 +316,12 @@ static void calc_wb_limits(struct rq_wb *rwb)
 		rwb->wb_normal = (rwb->rq_depth.max_depth + 1) / 2;
 		rwb->wb_background = (rwb->rq_depth.max_depth + 3) / 4;
 	}
+	trace_printk("%s[%d] --- rwb->wb_normal=%d, rwb->wb_background=%d, rwb->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, rwb->wb_normal, rwb->wb_background, rwb->min_lat_nsec);
 }
 
 static void scale_up(struct rq_wb *rwb)
 {
+	trace_printk("%s[%d] --- ", __FUNCTION__, __LINE__);
 	rq_depth_scale_up(&rwb->rq_depth);
 	calc_wb_limits(rwb);
 	rwb->unknown_cnt = 0;
@@ -316,6 +331,7 @@ static void scale_up(struct rq_wb *rwb)
 
 static void scale_down(struct rq_wb *rwb, bool hard_throttle)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	rq_depth_scale_down(&rwb->rq_depth, hard_throttle);
 	calc_wb_limits(rwb);
 	rwb->unknown_cnt = 0;
@@ -326,6 +342,7 @@ static void rwb_arm_timer(struct rq_wb *rwb)
 {
 	struct rq_depth *rqd = &rwb->rq_depth;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (rqd->scale_step > 0) {
 		/*
 		 * We should speed this up, using some variant of a fast
@@ -353,6 +370,10 @@ static void wb_timer_fn(struct blk_stat_callback *cb)
 	unsigned int inflight = wbt_inflight(rwb);
 	int status;
 
+	//dump_stack();
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	printk(KERN_ERR "%s[%d] --- scale_step=%d, inflight=%d\n", __FUNCTION__, __LINE__, rqd->scale_step, inflight);
+
 	status = latency_exceeded(rwb, cb->stat);
 
 	trace_wbt_timer(rwb->rqos.q->backing_dev_info, status, rqd->scale_step,
@@ -365,9 +386,11 @@ static void wb_timer_fn(struct blk_stat_callback *cb)
 	 */
 	switch (status) {
 	case LAT_EXCEEDED:
+        printk(KERN_ERR "%s[%d] --- exceeded, scale_down\n", __FUNCTION__, __LINE__);
 		scale_down(rwb, true);
 		break;
 	case LAT_OK:
+        printk(KERN_ERR "%s[%d] --- lat ok, scale_up\n", __FUNCTION__, __LINE__);
 		scale_up(rwb);
 		break;
 	case LAT_UNKNOWN_WRITES:
@@ -376,9 +399,11 @@ static void wb_timer_fn(struct blk_stat_callback *cb)
 		 * read/write sample, but we do have writes going on.
 		 * Allow step to go negative, to increase write perf.
 		 */
+        printk(KERN_ERR "%s[%d] --- LAT_UNKNOWN_WRITES, scale_up\n", __FUNCTION__, __LINE__);
 		scale_up(rwb);
 		break;
 	case LAT_UNKNOWN:
+        printk(KERN_ERR "%s[%d] --- LAT_UNKNOWN, scale_up\n", __FUNCTION__, __LINE__);
 		if (++rwb->unknown_cnt < RWB_UNKNOWN_BUMP)
 			break;
 		/*
@@ -436,6 +461,7 @@ void wbt_set_min_lat(struct request_queue *q, u64 val)
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (!rqos)
 		return;
+	trace_printk("%s[%d] --- ", __FUNCTION__, __LINE__);
 	RQWB(rqos)->min_lat_nsec = val;
 	RQWB(rqos)->enable_state = WBT_STATE_ON_MANUAL;
 	__wbt_update_limits(RQWB(rqos));
@@ -446,6 +472,7 @@ static bool close_io(struct rq_wb *rwb)
 {
 	const unsigned long now = jiffies;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	return time_before(now, rwb->last_issue + HZ / 10) ||
 		time_before(now, rwb->last_comp + HZ / 10);
 }
@@ -500,6 +527,7 @@ struct wbt_wait_data {
 static int wbt_wake_function(struct wait_queue_entry *curr, unsigned int mode,
 			     int wake_flags, void *key)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct wbt_wait_data *data = container_of(curr, struct wbt_wait_data,
 							wq);
 
@@ -525,6 +553,7 @@ static void __wbt_wait(struct rq_wb *rwb, enum wbt_flags wb_acct,
 	__releases(lock)
 	__acquires(lock)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_wait *rqw = get_rq_wait(rwb, wb_acct);
 	struct wbt_wait_data data = {
 		.wq = {
@@ -576,6 +605,7 @@ static void __wbt_wait(struct rq_wb *rwb, enum wbt_flags wb_acct,
 
 static inline bool wbt_should_throttle(struct rq_wb *rwb, struct bio *bio)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	switch (bio_op(bio)) {
 	case REQ_OP_WRITE:
 		/*
@@ -629,6 +659,7 @@ static void wbt_wait(struct rq_qos *rqos, struct bio *bio, spinlock_t *lock)
 	struct rq_wb *rwb = RQWB(rqos);
 	enum wbt_flags flags;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	flags = bio_to_wbt_flags(rwb, bio);
 	if (!(flags & WBT_TRACKED)) {
 		if (flags & WBT_READ)
@@ -644,6 +675,7 @@ static void wbt_wait(struct rq_qos *rqos, struct bio *bio, spinlock_t *lock)
 
 static void wbt_track(struct rq_qos *rqos, struct request *rq, struct bio *bio)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_wb *rwb = RQWB(rqos);
 	rq->wbt_flags |= bio_to_wbt_flags(rwb, bio);
 }
@@ -652,6 +684,7 @@ void wbt_issue(struct rq_qos *rqos, struct request *rq)
 {
 	struct rq_wb *rwb = RQWB(rqos);
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (!rwb_enabled(rwb))
 		return;
 
@@ -681,6 +714,7 @@ void wbt_requeue(struct rq_qos *rqos, struct request *rq)
 
 void wbt_set_queue_depth(struct request_queue *q, unsigned int depth)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (rqos) {
 		RQWB(rqos)->rq_depth.queue_depth = depth;
@@ -690,6 +724,7 @@ void wbt_set_queue_depth(struct request_queue *q, unsigned int depth)
 
 void wbt_set_write_cache(struct request_queue *q, bool write_cache_on)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (rqos)
 		RQWB(rqos)->wc = write_cache_on;
@@ -717,6 +752,7 @@ EXPORT_SYMBOL_GPL(wbt_enable_default);
 
 u64 wbt_default_latency_nsec(struct request_queue *q)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/*
 	 * We default to 2msec for non-rotational storage, and 75msec
 	 * for rotational storage.
@@ -731,6 +767,7 @@ static int wbt_data_dir(const struct request *rq)
 {
 	const int op = req_op(rq);
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (op == REQ_OP_READ)
 		return READ;
 	else if (op_is_write(op))
@@ -757,6 +794,7 @@ void wbt_disable_default(struct request_queue *q)
 {
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	struct rq_wb *rwb;
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (!rqos)
 		return;
 	rwb = RQWB(rqos);
@@ -781,6 +819,7 @@ int wbt_init(struct request_queue *q)
 	struct rq_wb *rwb;
 	int i;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	rwb = kzalloc(sizeof(*rwb), GFP_KERNEL);
 	if (!rwb)
 		return -ENOMEM;
diff --git a/block/genhd.c b/block/genhd.c
index be5bab20b2ab..0f2f12003ccd 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -819,12 +819,17 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 {
 	struct gendisk *disk = NULL;
 
+	printk(KERN_ERR "get_gendisk: major=%d", MAJOR(devt));
 	if (MAJOR(devt) != BLOCK_EXT_MAJOR) {
 		struct kobject *kobj;
 
 		kobj = kobj_lookup(bdev_map, devt, partno);
 		if (kobj)
 			disk = dev_to_disk(kobj_to_dev(kobj));
+		if (!kobj)
+			printk(KERN_ERR "get_gendisk: null kobj");
+		if (!disk)
+			printk(KERN_ERR "get_gendisk: null disk");
 	} else {
 		struct hd_struct *part;
 
@@ -837,6 +842,8 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 		spin_unlock_bh(&ext_devt_lock);
 	}
 
+	if (disk)
+		printk(KERN_ERR "get_gendisk: disk name=%s", disk->disk_name);
 	if (!disk)
 		return NULL;
 
diff --git a/fs/buffer.c b/fs/buffer.c
index 6f1ae3ac9789..9b1cb32d27aa 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -2985,6 +2985,7 @@ static void end_bio_bh_io_sync(struct bio *bio)
  * This allows us to do IO even on the odd last sectors
  * of a device, even if the block size is some multiple
  * of the physical sector size.
+ * 这允许我们在设备的奇数个最后扇区上执行IO偶数，即使块大小是物理扇区大小的几倍。
  *
  * We'll just truncate the bio to the size of the device,
  * and clear the end of the buffer head manually.
diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index db7590178dfc..7bcccc0c1307 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -299,6 +299,14 @@ static void ext4_end_bio(struct bio *bio)
 	ext4_io_end_t *io_end = bio->bi_private;
 	sector_t bi_sector = bio->bi_iter.bi_sector;
 	char b[BDEVNAME_SIZE];
+    static int dump_cnt = 0;
+
+    if (dump_cnt > 300) {
+        printk(KERN_ERR "%s[%d]: ---\n", __FUNCTION__, __LINE__);
+        dump_stack();
+        dump_cnt = 0;
+    } else
+        dump_cnt = 0;
 
 	if (WARN_ONCE(!io_end, "io_end is NULL: %s: sector %Lu len %u err %d\n",
 		      bio_devname(bio, b),
diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index 382c1ef9a9e4..cfebd290e399 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2335,6 +2335,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if ((f2fs_is_atomic_file(inode) &&
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||
 			is_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		drop_atomic = true;
 		goto fail;
@@ -2358,6 +2359,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	page = f2fs_pagecache_get_page(mapping, index,
 				FGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);
 	if (!page) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		goto fail;
 	}
@@ -2429,6 +2431,7 @@ static int f2fs_write_end(struct file *file,
 {
 	struct inode *inode = page->mapping->host;
 
+    trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	/*
diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index 471d863958bc..ca5516906a3e 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -245,6 +245,7 @@ void __inode_attach_wb(struct inode *inode, struct page *page)
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 	struct bdi_writeback *wb = NULL;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode_cgwb_enabled(inode)) {
 		struct cgroup_subsys_state *memcg_css;
 
@@ -283,6 +284,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 	__releases(&inode->i_lock)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	while (true) {
 		struct bdi_writeback *wb = inode_to_wb(inode);
 
@@ -319,6 +321,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 static struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	return locked_inode_to_wb_and_lock_list(inode);
 }
@@ -363,6 +366,7 @@ static void inode_switch_wbs_work_fn(struct work_struct *work)
 	spin_lock(&inode->i_lock);
 	xa_lock_irq(&mapping->i_pages);
 
+	trace_printk("%s[%d] CG-WB --- will down_read\n", __FUNCTION__, __LINE__);
 	/*
 	 * Once I_FREEING is visible under i_lock, the eviction path owns
 	 * the inode and we shouldn't modify ->i_io_list.
@@ -452,6 +456,7 @@ static void inode_switch_wbs_rcu_fn(struct rcu_head *rcu_head)
 	struct inode_switch_wbs_context *isw = container_of(rcu_head,
 				struct inode_switch_wbs_context, rcu_head);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* needs to grab bh-unsafe locks, bounce to work item */
 	INIT_WORK(&isw->work, inode_switch_wbs_work_fn);
 	queue_work(isw_wq, &isw->work);
@@ -471,6 +476,7 @@ static void inode_switch_wbs(struct inode *inode, int new_wb_id)
 	struct cgroup_subsys_state *memcg_css;
 	struct inode_switch_wbs_context *isw;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* noop if seems to be already in progress */
 	if (inode->i_state & I_WB_SWITCH)
 		return;
@@ -537,6 +543,7 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 		return;
 	}
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	wbc->wb = inode_to_wb(inode);
 	wbc->inode = inode;
 
@@ -603,6 +610,7 @@ void wbc_detach_inode(struct writeback_control *wbc)
 	u16 history;
 	int max_id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!wb)
 		return;
 
@@ -692,6 +700,7 @@ void wbc_account_io(struct writeback_control *wbc, struct page *page,
 {
 	int id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
@@ -739,6 +748,8 @@ EXPORT_SYMBOL_GPL(wbc_account_io);
  */
 int inode_congested(struct inode *inode, int cong_bits)
 {
+	static int cnt = 0;
+
 	/*
 	 * Once set, ->i_wb never becomes NULL while the inode is alive.
 	 * Start transaction iff ->i_wb is visible.
@@ -750,6 +761,11 @@ int inode_congested(struct inode *inode, int cong_bits)
 
 		wb = unlocked_inode_to_wb_begin(inode, &lock_cookie);
 		congested = wb_congested(wb, cong_bits);
+		if (cnt > 200) {
+			trace_printk("%s[%d] CG-WB --- congested=%d\n", __FUNCTION__, __LINE__, congested);
+			cnt = 0;
+		} else 
+			cnt ++;
 		unlocked_inode_to_wb_end(inode, &lock_cookie);
 		return congested;
 	}
@@ -772,6 +788,7 @@ static long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)
 	unsigned long this_bw = wb->avg_write_bandwidth;
 	unsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (nr_pages == LONG_MAX)
 		return LONG_MAX;
 
@@ -805,6 +822,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
 	struct bdi_writeback *wb = list_entry(&bdi->wb_list,
 					      struct bdi_writeback, bdi_node);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	might_sleep();
 restart:
 	rcu_read_lock();
@@ -877,6 +895,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
  */
 void cgroup_writeback_umount(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (atomic_read(&isw_nr_in_flight)) {
 		synchronize_rcu();
 		flush_workqueue(isw_wq);
@@ -885,6 +904,7 @@ void cgroup_writeback_umount(void)
 
 static int __init cgroup_writeback_init(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	isw_wq = alloc_workqueue("inode_switch_wbs", 0, 0);
 	if (!isw_wq)
 		return -ENOMEM;
@@ -1510,6 +1530,7 @@ static long writeback_sb_inodes(struct super_block *sb,
 	long write_chunk;
 	long wrote = 0;  /* count both pages and inodes */
 
+	trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (!list_empty(&wb->b_io)) {
 		struct inode *inode = wb_inode(wb->b_io.prev);
 		struct bdi_writeback *tmp_wb;
@@ -1654,7 +1675,7 @@ static long __writeback_inodes_wb(struct bdi_writeback *wb,
 			continue;
 		}
 		wrote += writeback_sb_inodes(sb, wb, work);
-		up_read(&sb->s_umount);
+		up_read(&sb->s_umount); //release a read lock
 
 		/* refer to the same tests at the end of writeback_sb_inodes */
 		if (wrote) {
@@ -1759,7 +1780,7 @@ static long wb_writeback(struct bdi_writeback *wb,
 		trace_writeback_start(wb, work);
 		if (list_empty(&wb->b_io))
 			queue_io(wb, work);
-		if (work->sb)
+		if (work->sb) //superblock
 			progress = writeback_sb_inodes(work->sb, wb, work);
 		else
 			progress = __writeback_inodes_wb(wb, work);
@@ -1878,6 +1899,7 @@ static long wb_check_start_all(struct bdi_writeback *wb)
 		return 0;
 
 	nr_pages = get_nr_dirty_pages();
+	trace_printk("CX____ wb dirty nr_pages=%ld\n", nr_pages);
 	if (nr_pages) {
 		struct wb_writeback_work work = {
 			.nr_pages	= wb_split_bdi_pages(wb, nr_pages),
@@ -1903,7 +1925,9 @@ static long wb_do_writeback(struct bdi_writeback *wb)
 	long wrote = 0;
 
 	set_bit(WB_writeback_running, &wb->state);
+    //writeback队列？
 	while ((work = get_next_work_item(wb)) != NULL) {
+		trace_printk("CX____ wb do next work, for_forground=%d, kupdate=%d\n", work->for_background, work->for_kupdate);
 		trace_writeback_exec(wb, work);
 		wrote += wb_writeback(wb, work);
 		finish_writeback_work(wb, work);
@@ -1911,6 +1935,7 @@ static long wb_do_writeback(struct bdi_writeback *wb)
 
 	/*
 	 * Check for a flush-everything request
+     * 设置了WB_start_all
 	 */
 	wrote += wb_check_start_all(wb);
 
diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 9a6bc0951cfa..560284f17c83 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -237,6 +237,7 @@ struct wb_lock_cookie {
  */
 static inline bool wb_tryget(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		return percpu_ref_tryget(&wb->refcnt);
 	return true;
@@ -248,6 +249,7 @@ static inline bool wb_tryget(struct bdi_writeback *wb)
  */
 static inline void wb_get(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		percpu_ref_get(&wb->refcnt);
 }
@@ -258,6 +260,7 @@ static inline void wb_get(struct bdi_writeback *wb)
  */
 static inline void wb_put(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		percpu_ref_put(&wb->refcnt);
 }
@@ -270,6 +273,7 @@ static inline void wb_put(struct bdi_writeback *wb)
  */
 static inline bool wb_dying(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return percpu_ref_is_dying(&wb->refcnt);
 }
 
diff --git a/include/linux/backing-dev.h b/include/linux/backing-dev.h
index c28a47cbe355..cd78ccd2a07a 100644
--- a/include/linux/backing-dev.h
+++ b/include/linux/backing-dev.h
@@ -248,6 +248,7 @@ static inline bool inode_cgwb_enabled(struct inode *inode)
 {
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return cgroup_subsys_on_dfl(memory_cgrp_subsys) &&
 		cgroup_subsys_on_dfl(io_cgrp_subsys) &&
 		bdi_cap_account_dirty(bdi) &&
@@ -268,6 +269,7 @@ static inline struct bdi_writeback *wb_find_current(struct backing_dev_info *bdi
 	struct cgroup_subsys_state *memcg_css;
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	memcg_css = task_css(current, memory_cgrp_id);
 	if (!memcg_css->parent)
 		return &bdi->wb;
@@ -297,6 +299,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
 {
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	wb = wb_find_current(bdi);
 	if (wb && unlikely(!wb_tryget(wb)))
@@ -322,6 +325,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
  */
 static inline bool inode_to_wb_is_valid(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return inode->i_wb;
 }
 
@@ -335,6 +339,7 @@ static inline bool inode_to_wb_is_valid(struct inode *inode)
  */
 static inline struct bdi_writeback *inode_to_wb(const struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_LOCKDEP
 	WARN_ON_ONCE(debug_locks &&
 		     (!lockdep_is_held(&inode->i_lock) &&
@@ -364,6 +369,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 {
 	rcu_read_lock();
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Paired with store_release in inode_switch_wb_work_fn() and
 	 * ensures that we see the new wb if we see cleared I_WB_SWITCH.
@@ -388,6 +394,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 static inline void unlocked_inode_to_wb_end(struct inode *inode,
 					    struct wb_lock_cookie *cookie)
 {
+	//trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (unlikely(cookie->locked))
 		xa_unlock_irqrestore(&inode->i_mapping->i_pages, cookie->flags);
 
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index 6d766a19f2bb..d251822796ff 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -253,6 +253,8 @@ static inline bool blk_cgroup_congested(void)
 	struct cgroup_subsys_state *css;
 	bool ret = false;
 
+	//CX____
+	//dump_stack();
 	rcu_read_lock();
 	css = kthread_blkcg();
 	if (!css)
@@ -399,6 +401,7 @@ extern void blkcg_destroy_blkgs(struct blkcg *blkcg);
  */
 static inline void blkcg_cgwb_get(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	refcount_inc(&blkcg->cgwb_refcnt);
 }
 
@@ -414,6 +417,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg)
  */
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (refcount_dec_and_test(&blkcg->cgwb_refcnt))
 		blkcg_destroy_blkgs(blkcg);
 }
@@ -424,6 +428,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg) { }
 
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] no CG-WB --- !!!!!!\n", __FUNCTION__, __LINE__);
 	/* wb isn't being accounted, so trigger destruction right away */
 	blkcg_destroy_blkgs(blkcg);
 }
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index f6dfb30737d8..089773244c6e 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -135,6 +135,8 @@ static inline void bio_issue_init(struct bio_issue *issue,
 	issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
 			(ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
 			((u64)size << BIO_ISSUE_SIZE_SHIFT));
+    //printk(KERN_ERR "CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
+    //trace_printk("CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
 }
 
 /*
diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e348f6..bba7029e0e20 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -203,6 +203,7 @@ void cgroup_writeback_umount(void);
  */
 static inline void inode_attach_wb(struct inode *inode, struct page *page)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!inode->i_wb)
 		__inode_attach_wb(inode, page);
 }
@@ -215,6 +216,7 @@ static inline void inode_attach_wb(struct inode *inode, struct page *page)
  */
 static inline void inode_detach_wb(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode->i_wb) {
 		WARN_ON_ONCE(!(inode->i_state & I_CLEAR));
 		wb_put(inode->i_wb);
@@ -234,6 +236,7 @@ static inline void inode_detach_wb(struct inode *inode)
 static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
 					       struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	inode_attach_wb(inode, NULL);
 	wbc_attach_and_unlock_inode(wbc, inode);
@@ -250,6 +253,7 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
diff --git a/include/trace/events/writeback.h b/include/trace/events/writeback.h
index 32db72c7c055..748da2e1cdc6 100644
--- a/include/trace/events/writeback.h
+++ b/include/trace/events/writeback.h
@@ -137,11 +137,13 @@ DEFINE_EVENT(writeback_dirty_inode_template, writeback_dirty_inode,
 
 static inline unsigned int __trace_wb_assign_cgroup(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return wb->memcg_css->cgroup->kn->id.ino;
 }
 
 static inline unsigned int __trace_wbc_assign_cgroup(struct writeback_control *wbc)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wbc->wb)
 		return __trace_wb_assign_cgroup(wbc->wb);
 	else
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index 4a3dae2a8283..6173588db80a 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -2655,6 +2655,7 @@ struct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)
 	if (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)
 		return ERR_PTR(-EINVAL);
 
+    trace_printk("CX____ %s[%d]: pid=%d\n", __FUNCTION__, __LINE__, pid);
 	percpu_down_write(&cgroup_threadgroup_rwsem);
 
 	rcu_read_lock();
diff --git a/mm/filemap.c b/mm/filemap.c
index 52517f28e6f4..001a50f90e85 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -2089,6 +2089,8 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		if (!page) {
 			if (iocb->ki_flags & IOCB_NOWAIT)
 				goto would_block;
+            /*should be called when a cache miss happened: it will submit the read. The readahead logic may decide to piggyback more pages onto the read request if access patterns suggest it will improve performance.*/
+            /*发生缓存未命中时应调用：它将提交读取。如果访问模式表明readahead逻辑可以提高性能，那么readahead逻辑可能会决定将更多的页面装载到read请求上。*/
 			page_cache_sync_readahead(mapping,
 					ra, filp,
 					index, last_index - index);
@@ -2097,6 +2099,8 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 				goto no_cached_page;
 		}
 		if (PageReadahead(page)) {
+            /*a page is used which has the PG_readahead flag; this is a marker to suggest that the application has used up enough of the readahead window that we should start pulling in more pages.*/
+            /*当使用带有PG_readahead标志的页面时应该调用；这是一个标记，表示应用程序已经用完了足够的readahead窗口，我们应该开始拉入更多页面*/
 			page_cache_async_readahead(mapping,
 					ra, filp, page,
 					index, last_index - index);
diff --git a/mm/readahead.c b/mm/readahead.c
index 4e630143a0ba..7b3103fd0626 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -393,6 +393,7 @@ ondemand_readahead(struct address_space *mapping,
 	/*
 	 * If the request exceeds the readahead window, allow the read to
 	 * be up to the optimal hardware IO size
+     * 如果请求超过readahead窗口，则允许读取达到最佳硬件IO大小
 	 */
 	if (req_size > max_pages && bdi->io_pages > max_pages)
 		max_pages = min(req_size, bdi->io_pages);
@@ -406,6 +407,7 @@ ondemand_readahead(struct address_space *mapping,
 	/*
 	 * It's the expected callback offset, assume sequential access.
 	 * Ramp up sizes, and push forward the readahead window.
+     * 这是预期的回调偏移量，假设是顺序访问。加大尺寸，并向前推readahead窗口。
 	 */
 	if ((offset == (ra->start + ra->size - ra->async_size) ||
 	     offset == (ra->start + ra->size))) {
@@ -515,11 +517,14 @@ void page_cache_sync_readahead(struct address_space *mapping,
 	if (!ra->ra_pages)
 		return;
 
-	if (blk_cgroup_congested())
+	if (blk_cgroup_congested()) {
+        trace_printk("%s[%d]: skip sync readahead, congested\n", __FUNCTION__, __LINE__);
 		return;
+    }
 
 	/* be dumb */
 	if (filp && (filp->f_mode & FMODE_RANDOM)) {
+        trace_printk("%s[%d]: FMODE_RAMDOM \n", __FUNCTION__, __LINE__);
 		force_page_cache_readahead(mapping, filp, offset, req_size);
 		return;
 	}
@@ -565,11 +570,15 @@ page_cache_async_readahead(struct address_space *mapping,
 	/*
 	 * Defer asynchronous read-ahead on IO congestion.
 	 */
-	if (inode_read_congested(mapping->host))
+	if (inode_read_congested(mapping->host)) {
+        trace_printk("%s[%d]: skip readahead, inode read congested\n", __FUNCTION__, __LINE__);
 		return;
+    }
 
-	if (blk_cgroup_congested())
+	if (blk_cgroup_congested()) {
+        trace_printk("%s[%d]: skip async readahead, congested\n", __FUNCTION__, __LINE__);
 		return;
+    }
 
 	/* do read-ahead */
 	ondemand_readahead(mapping, ra, filp, true, offset, req_size);
