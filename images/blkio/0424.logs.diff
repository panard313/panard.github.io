diff --git a/Makefile b/Makefile
index f1e428271..962d31505 100644
--- a/Makefile
+++ b/Makefile
@@ -662,7 +662,7 @@ else
 ifdef CONFIG_PROFILE_ALL_BRANCHES
 KBUILD_CFLAGS	+= -O2 $(call cc-disable-warning,maybe-uninitialized,)
 else
-KBUILD_CFLAGS   += -O2
+KBUILD_CFLAGS   += -O1
 endif
 endif
 
diff --git a/block/bio.c b/block/bio.c
index 3d7570553..38f42892f 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -2001,7 +2001,7 @@ int bio_associate_blkcg_from_page(struct bio *bio, struct page *page)
 int bio_associate_blkcg(struct bio *bio, struct cgroup_subsys_state *blkcg_css)
 {
 	if (unlikely(bio->bi_css))
-		return -EBUSY;
+		return -EBUSY; //CX____ may cost time?
 	css_get(blkcg_css);
 	bio->bi_css = blkcg_css;
 	return 0;
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index a06547fe6..b98541213 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -821,6 +821,7 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	int key_len, part, ret;
 	char *body;
 
+	printk(KERN_ERR "blkg_conf_prep %s", input);
 	if (sscanf(input, "%u:%u%n", &major, &minor, &key_len) != 2)
 		return -EINVAL;
 
@@ -830,8 +831,11 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	body = skip_spaces(body);
 
 	disk = get_gendisk(MKDEV(major, minor), &part);
-	if (!disk)
+	if (!disk) {
+		printk(KERN_ERR "blkg_conf_prep null diks");
 		return -ENODEV;
+	}
+	printk(KERN_ERR "blkg_conf_prep part=%d", part);
 	if (part) {
 		ret = -ENODEV;
 		goto fail;
@@ -1626,6 +1630,7 @@ static void blkcg_scale_delay(struct blkcg_gq *blkg, u64 now)
 {
 	u64 old = atomic64_read(&blkg->delay_start);
 
+    trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/*
 	 * We only want to scale down every second.  The idea here is that we
 	 * want to delay people for min(delay_nsec, NSEC_PER_SEC) in a certain
@@ -1682,6 +1687,7 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	u64 delay_nsec = 0;
 	int tok;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	while (blkg->parent) {
 		if (atomic_read(&blkg->use_delay)) {
 			blkcg_scale_delay(blkg, now);
@@ -1700,6 +1706,8 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	 * delay, and we want userspace to be able to do _something_ so cap the
 	 * delays at 1 second.  If there's 10's of seconds worth of delay then
 	 * the tasks will be delayed for 1 second for every syscall.
+	 * 如果我们耽搁了太久，就不要睡到永远。
+交换或元数据IO可以累积10秒的延迟，我们希望用户空间能够做些什么，所以将延迟限制在1秒。如果有10秒的延迟，那么每个系统调用的任务将延迟1秒。
 	 */
 	delay_nsec = min_t(u64, delay_nsec, 250 * NSEC_PER_MSEC);
 
@@ -1709,6 +1717,9 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	 * to do a psi_memstall_enter/leave if memdelay is set.
 	 */
 
+	trace_printk("%s[%d] --- delay_nsec=%lly\n", __FUNCTION__, __LINE__, delay_nsec);
+	printk(KERN_ERR "%s[%d] --- delay_nsec=%lly\n", __FUNCTION__, __LINE__, delay_nsec);
+
 	exp = ktime_add_ns(now, delay_nsec);
 	tok = io_schedule_prepare();
 	do {
@@ -1728,6 +1739,7 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
  * anything.  This should only ever be called by the resume code, it's not meant
  * to be called by people willy-nilly as it will actually do the work to
  * throttle the task if it is setup for throttling.
+ * blkcg_maybe_throttle_current-如果当前任务已标记，则对其进行限制。仅当我们被标记为set_notify_resume（）时才调用此函数。显然，我们可以因为blkcg throttling以外的原因设置notify_resume（），所以我们检查current->throttling_queue是否已设置，如果未设置，则不会执行任何操作。这应该只由恢复代码调用，而不是由人们随意调用，因为如果设置为限制任务，它将实际执行限制任务的工作。
  */
 void blkcg_maybe_throttle_current(void)
 {
@@ -1740,6 +1752,7 @@ void blkcg_maybe_throttle_current(void)
 	if (!q)
 		return;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	current->throttle_queue = NULL;
 	current->use_memdelay = false;
 
@@ -1795,8 +1808,10 @@ void blkcg_schedule_throttle(struct request_queue *q, bool use_memdelay)
 	if (!blk_get_queue(q))
 		return;
 
-	if (current->throttle_queue)
-		blk_put_queue(current->throttle_queue);
+	if (current->throttle_queue) {
+		trace_printk("%s[%d] --- current->throttle_queue.id=%d\n", __FUNCTION__, __LINE__, current->throttle_queue->id);
+		blk_put_queue(current->throttle_queue); //减小引用？
+	}
 	current->throttle_queue = q;
 	if (use_memdelay)
 		current->use_memdelay = use_memdelay;
@@ -1816,6 +1831,7 @@ void blkcg_add_delay(struct blkcg_gq *blkg, u64 now, u64 delta)
 {
 	blkcg_scale_delay(blkg, now);
 	atomic64_add(delta, &blkg->delay_nsec);
+	trace_printk("CX____ %s[%d] add %llu (ns), to %llu (ns)\n", __FUNCTION__, __LINE__, delta, blkg->delay_nsec);
 }
 EXPORT_SYMBOL_GPL(blkcg_add_delay);
 
diff --git a/block/blk-core.c b/block/blk-core.c
index ea33d6abd..29147bc71 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -145,7 +145,9 @@ EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_clear);
 
 static void blk_clear_congested(struct request_list *rl, int sync)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	clear_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/*
@@ -159,7 +161,9 @@ static void blk_clear_congested(struct request_list *rl, int sync)
 
 static void blk_set_congested(struct request_list *rl, int sync)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	set_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/* see blk_clear_congested() */
@@ -172,15 +176,18 @@ void blk_queue_congestion_threshold(struct request_queue *q)
 {
 	int nr;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	nr = q->nr_requests - (q->nr_requests / 8) + 1;
 	if (nr > q->nr_requests)
 		nr = q->nr_requests;
 	q->nr_congestion_on = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_on=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_on);
 
 	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
 	if (nr < 1)
 		nr = 1;
 	q->nr_congestion_off = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_off=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_off);
 }
 
 void blk_rq_init(struct request_queue *q, struct request *rq)
@@ -1362,6 +1369,7 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	const bool is_sync = op_is_sync(op);
 	int may_queue;
 	req_flags_t rq_flags = RQF_ALLOCED;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 
@@ -1473,6 +1481,8 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	if (ioc_batching(q, ioc))
 		ioc->nr_batch_requests--;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_getrq(q, bio, op);
 	return rq;
 
@@ -1540,6 +1550,7 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 	DEFINE_WAIT(wait);
 	struct request_list *rl;
 	struct request *rq;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
@@ -1564,6 +1575,8 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 	prepare_to_wait_exclusive(&rl->wait[is_sync], &wait,
 				  TASK_UNINTERRUPTIBLE);
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_sleeprq(q, bio, op);
 
 	spin_unlock_irq(q->queue_lock);
@@ -1653,11 +1666,15 @@ EXPORT_SYMBOL(blk_get_request);
  */
 void blk_requeue_request(struct request_queue *q, struct request *rq)
 {
+	struct task_struct *tsk = current;
+
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
 	blk_delete_timer(rq);
 	blk_clear_rq_complete(rq);
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_rq_requeue(q, rq);
 	rq_qos_requeue(q, rq);
 
@@ -1806,11 +1823,14 @@ EXPORT_SYMBOL(blk_put_request);
 bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 			    struct bio *bio)
 {
+	struct task_struct *tsk = current;
 	const int ff = bio->bi_opf & REQ_FAILFAST_MASK;
 
 	if (!ll_back_merge_fn(q, req, bio))
 		return false;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_bio_backmerge(q, req, bio);
 
 	if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
@@ -1828,11 +1848,14 @@ bool bio_attempt_back_merge(struct request_queue *q, struct request *req,
 bool bio_attempt_front_merge(struct request_queue *q, struct request *req,
 			     struct bio *bio)
 {
+	struct task_struct *tsk = current;
 	const int ff = bio->bi_opf & REQ_FAILFAST_MASK;
 
 	if (!ll_front_merge_fn(q, req, bio))
 		return false;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_bio_frontmerge(q, req, bio);
 
 	if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
@@ -2000,6 +2023,7 @@ static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)
 	int where = ELEVATOR_INSERT_SORT;
 	struct request *req, *free;
 	unsigned int request_count = 0;
+	struct task_struct *tsk = current;
 
 	/*
 	 * low level driver can indicate that it wants pages above a
@@ -2098,6 +2122,8 @@ static blk_qc_t blk_queue_bio(struct request_queue *q, struct bio *bio)
 		 * @request_count may become stale because of schedule
 		 * out, so check plug list again.
 		 */
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		if (!request_count || list_empty(&plug->list))
 			trace_block_plug(q);
 		else {
@@ -2221,6 +2247,7 @@ static inline int blk_partition_remap(struct bio *bio)
 {
 	struct hd_struct *p;
 	int ret = -EIO;
+	struct task_struct *tsk = current;
 
 	rcu_read_lock();
 	p = __disk_get_part(bio->bi_disk, bio->bi_partno);
@@ -2239,6 +2266,8 @@ static inline int blk_partition_remap(struct bio *bio)
 		if (bio_check_eod(bio, part_nr_sects_read(p)))
 			goto out;
 		bio->bi_iter.bi_sector += p->start_sect;
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		trace_block_bio_remap(bio->bi_disk->queue, bio, part_devt(p),
 				      bio->bi_iter.bi_sector - p->start_sect);
 	}
@@ -2256,6 +2285,7 @@ generic_make_request_checks(struct bio *bio)
 	int nr_sectors = bio_sectors(bio);
 	blk_status_t status = BLK_STS_IOERR;
 	char b[BDEVNAME_SIZE];
+	struct task_struct *tsk = current;
 
 	might_sleep();
 
@@ -2340,6 +2370,8 @@ generic_make_request_checks(struct bio *bio)
 		return false;
 
 	if (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {
+		if (tsk)
+			trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 		trace_block_bio_queue(q, bio);
 		/* Now that enqueuing has been traced, we need to trace
 		 * completion as well.
@@ -2876,6 +2908,7 @@ struct request *blk_peek_request(struct request_queue *q)
 {
 	struct request *rq;
 	int ret;
+	struct task_struct *tsk = current;
 
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
@@ -2896,6 +2929,8 @@ struct request *blk_peek_request(struct request_queue *q)
 			 * not be passed by new incoming requests
 			 */
 			rq->rq_flags |= RQF_STARTED;
+			if (tsk)
+				trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 			trace_block_rq_issue(q, rq);
 		}
 
@@ -3084,7 +3119,10 @@ bool blk_update_request(struct request *req, blk_status_t error,
 		unsigned int nr_bytes)
 {
 	int total_bytes;
+	struct task_struct *tsk = current;
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_rq_complete(req, blk_status_to_errno(error), nr_bytes);
 
 	if (!req->bio)
@@ -3636,8 +3674,11 @@ static void queue_unplugged(struct request_queue *q, unsigned int depth,
 			    bool from_schedule)
 	__releases(q->queue_lock)
 {
+	struct task_struct *tsk = current;
 	lockdep_assert_held(q->queue_lock);
 
+	if (tsk)
+		trace_printk("%s[%d] --- for blk: pid=%d\n", __FUNCTION__, __LINE__, tsk->pid);
 	trace_block_unplug(q, depth, !from_schedule);
 
 	if (from_schedule)
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index 0529e94a2..bd220aeda 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -79,6 +79,8 @@
 
 #define DEFAULT_SCALE_COOKIE 1000000U
 
+static int g_dump_stack_counter = 0;
+
 static struct blkcg_policy blkcg_policy_iolatency;
 struct iolatency_grp;
 
@@ -90,11 +92,13 @@ struct blk_iolatency {
 
 static inline struct blk_iolatency *BLKIOLATENCY(struct rq_qos *rqos)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return container_of(rqos, struct blk_iolatency, rqos);
 }
 
 static inline bool blk_iolatency_enabled(struct blk_iolatency *blkiolat)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return atomic_read(&blkiolat->enabled) > 0;
 }
 
@@ -179,11 +183,19 @@ static inline bool iolatency_may_queue(struct iolatency_grp *iolat,
 				       bool first_block)
 {
 	struct rq_wait *rqw = &iolat->rq_wait;
+    int ret = 0;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	/* waitqueue_active() returns true if the wait list is not empty */
 	if (first_block && waitqueue_active(&rqw->wait) &&
-	    rqw->wait.head.next != &wait->entry)
+	    rqw->wait.head.next != &wait->entry) {
+		trace_printk("%s[%d] --- return false\n", __FUNCTION__, __LINE__);
 		return false;
-	return rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	}
+	//return rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	ret = rq_wait_inc_below(rqw, iolat->rq_depth.max_depth);
+	trace_printk("%s[%d] --- max_depth=%llu\n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
+    return ret;
 }
 
 static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
@@ -195,24 +207,34 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 {
 	struct rq_wait *rqw = &iolat->rq_wait;
 	unsigned use_delay = atomic_read(&lat_to_blkg(iolat)->use_delay);
-	DEFINE_WAIT(wait);
+	DEFINE_WAIT(wait); // now we have a wait entry
 	bool first_block = true;
+    int tmp_inflight = 0;
 
-	if (use_delay)
-		blkcg_schedule_throttle(rqos->q, use_memdelay);
+	trace_printk("%s[%d] --- Entry, current latency=%llu\n", __FUNCTION__, __LINE__, iolat->min_lat_nsec);
+	if (use_delay) {
+		trace_printk("%s[%d] --- use_delay ^^^ use_memdelay=%d\n", __FUNCTION__, __LINE__, use_memdelay);
+		blkcg_schedule_throttle(rqos->q, use_memdelay); // use delay to memcg ?? check later
+	}
 
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 	/*
 	 * To avoid priority inversions we want to just take a slot if we are
 	 * issuing as root.  If we're being killed off there's no point in
 	 * delaying things, we may have been killed by OOM so throttling may
 	 * make recovery take even longer, so just let the IO's through so the
 	 * task can go away.
+	 * 为了避免优先级反转，如果我们以根用户身份发布，我们只需要占用一个插槽。如果我们被杀了，拖延时间是没有意义的，我们可能已经被OOM杀死了，所以节流可能会使恢复时间更长，所以让IO完成任务就可以了。
 	 */
 	if (issue_as_root || fatal_signal_pending(current)) {
+        tmp_inflight = atomic_read(&rqw->inflight);
 		atomic_inc(&rqw->inflight);
+        if (atomic_read(&rqw->inflight) != tmp_inflight)
+	        trace_printk("%s[%d] --- inflight chenged from %d to %d\n", __FUNCTION__, __LINE__, tmp_inflight, rqw->inflight);
 		return;
 	}
 
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 	if (iolatency_may_queue(iolat, &wait, first_block))
 		return;
 
@@ -220,12 +242,14 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 		prepare_to_wait_exclusive(&rqw->wait, &wait,
 					  TASK_UNINTERRUPTIBLE);
 
+	    trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 		if (iolatency_may_queue(iolat, &wait, first_block))
 			break;
 		first_block = false;
 
 		if (lock) {
 			spin_unlock_irq(lock);
+			trace_printk("%s[%d] --- io_schedule()\n", __FUNCTION__, __LINE__);
 			io_schedule();
 			spin_lock_irq(lock);
 		} else {
@@ -234,6 +258,8 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 	} while (1);
 
 	finish_wait(&rqw->wait, &wait);
+	trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
+	trace_printk("%s[%d] --- Eixt, current latency=%llu\n", __FUNCTION__, __LINE__, iolat->min_lat_nsec);
 }
 
 #define SCALE_DOWN_FACTOR 2
@@ -263,6 +289,7 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 	unsigned long max_scale = qd << 1;
 	unsigned long diff = 0;
 
+	trace_printk("%s[%d] --- entry, cookie=%d, up=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter, up);
 	if (old < DEFAULT_SCALE_COOKIE)
 		diff = DEFAULT_SCALE_COOKIE - old;
 
@@ -288,6 +315,7 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 			atomic_sub(scale, &lat_info->scale_cookie);
 		}
 	}
+	trace_printk("%s[%d] --- finish, cookie=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter);
 }
 
 /*
@@ -305,6 +333,7 @@ static void scale_change(struct iolatency_grp *iolat, bool up)
 	if (old > qd)
 		old = qd;
 
+	trace_printk("%s[%d] --- Entry, max_depth is %d , up=%d\n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth, up);
 	if (up) {
 		if (old == 1 && blkcg_unuse_delay(lat_to_blkg(iolat)))
 			return;
@@ -314,13 +343,16 @@ static void scale_change(struct iolatency_grp *iolat, bool up)
 			old += scale;
 			old = min(old, qd);
 			iolat->rq_depth.max_depth = old;
+			trace_printk("%s[%d] --- up, max_depth added %lu to %d \n", __FUNCTION__, __LINE__, scale, iolat->rq_depth.max_depth);
 			wake_up_all(&iolat->rq_wait.wait);
 		}
 	} else if (old > 1) {
 		old >>= 1;
 		changed = true;
 		iolat->rq_depth.max_depth = max(old, 1UL);
+		trace_printk("%s[%d] --- down, max_depth to %d \n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
 	}
+	trace_printk("%s[%d] --- Exit, max_depth to %d \n", __FUNCTION__, __LINE__, iolat->rq_depth.max_depth);
 }
 
 /* Check our parent and see if the scale cookie has changed. */
@@ -333,6 +365,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	u64 scale_lat;
 	unsigned int old;
 	int direction = 0;
+	static int count = 0;
 
 	if (lat_to_blkg(iolat)->parent == NULL)
 		return;
@@ -341,10 +374,18 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	if (!parent)
 		return;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	lat_info = &parent->child_lat;
 	cur_cookie = atomic_read(&lat_info->scale_cookie);
 	scale_lat = READ_ONCE(lat_info->scale_lat);
 
+	if (count > 300) {
+		trace_printk("%s[%d] --- scale_lat=%llu, iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, scale_lat, iolat->min_lat_nsec);
+		count = 0;
+	} else {
+		count++;
+	}
+
 	if (cur_cookie < our_cookie)
 		direction = -1;
 	else if (cur_cookie > our_cookie)
@@ -379,6 +420,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	/* We're as low as we can go. */
 	if (iolat->rq_depth.max_depth == 1 && direction < 0) {
 		blkcg_use_delay(lat_to_blkg(iolat));
+		trace_printk("%s[%d] --- we already as low as we can go !!!  rq_depth.max_depth==1\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
@@ -387,9 +429,11 @@ static void check_scale_change(struct iolatency_grp *iolat)
 		blkcg_clear_delay(lat_to_blkg(iolat));
 		iolat->rq_depth.max_depth = UINT_MAX;
 		wake_up_all(&iolat->rq_wait.wait);
+		trace_printk("%s[%d] --- We're back to the default cookie, unthrottle all the things.\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
+	trace_printk("%s[%d] --- scale\n", __FUNCTION__, __LINE__);
 	scale_change(iolat, direction > 0);
 }
 
@@ -402,12 +446,21 @@ static void blkcg_iolatency_throttle(struct rq_qos *rqos, struct bio *bio,
 	struct request_queue *q = rqos->q;
 	bool issue_as_root = bio_issue_as_root_blkg(bio);
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	if (!blk_iolatency_enabled(blkiolat))
 		return;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	blkcg = bio_blkcg(bio);
-	bio_associate_blkcg(bio, &blkcg->css);
+	bio_associate_blkcg(bio, &blkcg->css); // bio->bi_css = blkcg_css;
 	blkg = blkg_lookup(blkcg, q);
 	if (unlikely(!blkg)) {
 		if (!lock)
@@ -422,10 +475,16 @@ static void blkcg_iolatency_throttle(struct rq_qos *rqos, struct bio *bio,
 		goto out;
 
 	bio_issue_init(&bio->bi_issue, bio_sectors(bio));
-	bio_associate_blkg(bio, blkg);
+	bio_associate_blkg(bio, blkg); // bio->bi_blkg = blkg;
+	struct iolatency_grp *tmp_iolat = blkg_to_lat(blkg);
+    if (!tmp_iolat)
+	    trace_printk("%s[%d] --- bio->bi_issue=%llu\n", __FUNCTION__, __LINE__, bio->bi_issue.value);
+    else
+	    trace_printk("%s[%d] --- lat-%llu --- bio->bi_issue=%llu\n", __FUNCTION__, __LINE__, tmp_iolat->min_lat_nsec, bio->bi_issue.value);
+
 out:
 	rcu_read_unlock();
-	while (blkg && blkg->parent) {
+	while (blkg && blkg->parent) { //从下往上
 		struct iolatency_grp *iolat = blkg_to_lat(blkg);
 		if (!iolat) {
 			blkg = blkg->parent;
@@ -449,6 +508,7 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 	u64 start = bio_issue_time(issue);
 	u64 req_time;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Have to do this so we are truncated to the correct time that our
 	 * issue is truncated to.
@@ -468,11 +528,13 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 		u64 sub = iolat->min_lat_nsec;
 		if (req_time < sub)
 			blkcg_add_delay(lat_to_blkg(iolat), now, sub - req_time);
+			//trace_printk("%s[%d] --- root grp, add delay\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
 	rq_stat = get_cpu_ptr(iolat->stats);
 	blk_rq_stat_add(rq_stat, req_time);
+	trace_printk("%s[%d] --- blk_rq_stat_add=%llu\n", __FUNCTION__, __LINE__, req_time);
 	put_cpu_ptr(rq_stat);
 }
 
@@ -488,6 +550,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	unsigned long flags;
 	int cpu, exp_idx;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blk_rq_stat_init(&stat);
 	preempt_disable();
 	for_each_online_cpu(cpu) {
@@ -499,10 +562,14 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	preempt_enable();
 
 	parent = blkg_to_lat(blkg->parent);
-	if (!parent)
+	if (!parent) {
+		trace_printk("%s[%d] --- ignore root grp?\n", __FUNCTION__, __LINE__);
 		return;
+	}
 
 	lat_info = &parent->child_lat;
+	trace_printk("CX____ %s[%d]: lat_info->scale_lat=%lld\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
+	//printk(KERN_ERR "CX____ %s[%d]: lat_info->scale_lat=%lld", __FUNCTION__, __LINE__, lat_info->scale_lat);
 
 	/*
 	 * CALC_LOAD takes in a number stored in fixed point representation.
@@ -527,10 +594,13 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	lat_info->nr_samples += stat.nr_samples;
 	iolat->nr_samples = stat.nr_samples;
 
+    trace_printk("%s[%d]: scale_lat=%llu\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
 	if ((lat_info->last_scale_event >= now ||
 	    now - lat_info->last_scale_event < BLKIOLATENCY_MIN_ADJUST_TIME) &&
-	    lat_info->scale_lat <= iolat->min_lat_nsec)
+	    lat_info->scale_lat <= iolat->min_lat_nsec){
+        trace_printk("%s[%d]: no need, goto out\n", __FUNCTION__, __LINE__);
 		goto out;
+	}
 
 	if (stat.mean <= iolat->min_lat_nsec &&
 	    stat.nr_samples >= BLKIOLATENCY_MIN_GOOD_SAMPLES) {
@@ -545,6 +615,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 			WRITE_ONCE(lat_info->scale_lat, iolat->min_lat_nsec);
 			lat_info->scale_grp = iolat;
 		}
+		trace_printk("%s[%d] --- stat.mean=%llu > iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, stat.mean, iolat->min_lat_nsec);
 		scale_cookie_change(iolat->blkiolat, lat_info, false);
 	}
 out:
@@ -562,11 +633,15 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 	bool enabled = false;
 	int inflight = 0;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkg = bio->bi_blkg;
 	if (!blkg)
 		return;
 
 	iolat = blkg_to_lat(bio->bi_blkg);
+	if (!issue_as_root) {
+		trace_printk("CX____ %s[%d]\n", __FUNCTION__, __LINE__);
+	}
 	if (!iolat)
 		return;
 
@@ -582,7 +657,9 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 		}
 		rqw = &iolat->rq_wait;
 
+		// inflight - 1
 		inflight = atomic_dec_return(&rqw->inflight);
+	    trace_printk("%s[%d] --- trace inflight: %d\n", __FUNCTION__, __LINE__, rqw->inflight);
 		WARN_ON_ONCE(inflight < 0);
 		/*
 		 * If bi_status is BLK_STS_AGAIN, the bio wasn't actually
@@ -591,14 +668,17 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 		if (iolat->min_lat_nsec && bio->bi_status != BLK_STS_AGAIN) {
 			iolatency_record_time(iolat, &bio->bi_issue, now,
 					      issue_as_root);
+			trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 			window_start = atomic64_read(&iolat->window_start);
 			if (now > window_start &&
 			    (now - window_start) >= iolat->cur_win_nsec) {
 				if (atomic64_cmpxchg(&iolat->window_start,
 					     window_start, now) == window_start)
+					trace_printk("CX____ %s[%d] --- over window? will check latency  now-window_start=%llu\n", __FUNCTION__, __LINE__, now-window_start);
 					iolatency_check_latencies(iolat, now);
 			}
 		}
+		trace_printk("CX____ %s[%d] --- wake_up rqw->wait  !!!!!\n", __FUNCTION__, __LINE__);
 		wake_up(&rqw->wait);
 		blkg = blkg->parent;
 	}
@@ -608,6 +688,7 @@ static void blkcg_iolatency_exit(struct rq_qos *rqos)
 {
 	struct blk_iolatency *blkiolat = BLKIOLATENCY(rqos);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	del_timer_sync(&blkiolat->timer);
 	blkcg_deactivate_policy(rqos->q, &blkcg_policy_iolatency);
 	kfree(blkiolat);
@@ -626,6 +707,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	struct cgroup_subsys_state *pos_css;
 	u64 now = ktime_to_ns(ktime_get());
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	blkg_for_each_descendant_pre(blkg, pos_css,
 				     blkiolat->rqos.q->root_blkg) {
@@ -660,6 +742,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * on.
 		 */
 		if (lat_info->scale_grp == NULL) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			scale_cookie_change(iolat->blkiolat, lat_info, true);
 			goto next_lock;
 		}
@@ -670,11 +753,15 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * doing any IO currently.
 		 */
 		if (now - lat_info->last_scale_event >=
-		    ((u64)NSEC_PER_SEC * 5))
+		    ((u64)NSEC_PER_SEC * 5)) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			lat_info->scale_grp = NULL;
+		}
 next_lock:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		spin_unlock_irqrestore(&lat_info->lock, flags);
 next:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		blkg_put(blkg);
 	}
 	rcu_read_unlock();
@@ -686,6 +773,7 @@ int blk_iolatency_init(struct request_queue *q)
 	struct rq_qos *rqos;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkiolat = kzalloc(sizeof(*blkiolat), GFP_KERNEL);
 	if (!blkiolat)
 		return -ENOMEM;
@@ -718,10 +806,13 @@ static int iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 	struct iolatency_grp *iolat = blkg_to_lat(blkg);
 	u64 oldval = iolat->min_lat_nsec;
 
+	trace_printk("%s[%d] --- val=%llu\n", __FUNCTION__, __LINE__, val);
+	printk(KERN_ERR "%s[%d] --- val=%llu", __FUNCTION__, __LINE__, val);
 	iolat->min_lat_nsec = val;
 	iolat->cur_win_nsec = max_t(u64, val << 4, BLKIOLATENCY_MIN_WIN_SIZE);
 	iolat->cur_win_nsec = min_t(u64, iolat->cur_win_nsec,
 				    BLKIOLATENCY_MAX_WIN_SIZE);
+	printk(KERN_ERR "%s[%d] --- cur_win_nsec=%llu", __FUNCTION__, __LINE__, iolat->cur_win_nsec);
 
 	if (!oldval && val)
 		return 1;
@@ -734,6 +825,7 @@ static int iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 
 static void iolatency_clear_scaling(struct blkcg_gq *blkg)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (blkg->parent) {
 		struct iolatency_grp *iolat = blkg_to_lat(blkg->parent);
 		struct child_latency_info *lat_info;
@@ -764,6 +856,7 @@ static ssize_t iolatency_set_limit(struct kernfs_open_file *of, char *buf,
 	int ret;
 	int enable = 0;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	ret = blkg_conf_prep(blkcg, &blkcg_policy_iolatency, buf, &ctx);
 	if (ret)
 		return ret;
@@ -838,6 +931,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	const char *dname = blkg_dev_name(pd->blkg);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (!dname || !iolat->min_lat_nsec)
 		return 0;
 	seq_printf(sf, "%s target=%llu\n",
@@ -847,6 +941,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 
 static int iolatency_print_limit(struct seq_file *sf, void *v)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),
 			  iolatency_prfill_limit,
 			  &blkcg_policy_iolatency, seq_cft(sf)->private, false);
@@ -860,6 +955,7 @@ static size_t iolatency_pd_stat(struct blkg_policy_data *pd, char *buf,
 	unsigned long long avg_lat = div64_u64(iolat->lat_avg, NSEC_PER_USEC);
 	unsigned long long cur_win = div64_u64(iolat->cur_win_nsec, NSEC_PER_MSEC);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (iolat->rq_depth.max_depth == UINT_MAX)
 		return scnprintf(buf, size, " depth=max avg_lat=%llu win=%llu",
 				 avg_lat, cur_win);
@@ -873,6 +969,7 @@ static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp, int node)
 {
 	struct iolatency_grp *iolat;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	iolat = kzalloc_node(sizeof(*iolat), gfp, node);
 	if (!iolat)
 		return NULL;
@@ -894,6 +991,7 @@ static void iolatency_pd_init(struct blkg_policy_data *pd)
 	u64 now = ktime_to_ns(ktime_get());
 	int cpu;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		struct blk_rq_stat *stat;
 		stat = per_cpu_ptr(iolat->stats, cpu);
@@ -931,6 +1029,7 @@ static void iolatency_pd_offline(struct blkg_policy_data *pd)
 	struct blk_iolatency *blkiolat = iolat->blkiolat;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	ret = iolatency_set_min_lat_nsec(blkg, 0);
 	if (ret == 1)
 		atomic_inc(&blkiolat->enabled);
@@ -941,6 +1040,7 @@ static void iolatency_pd_offline(struct blkg_policy_data *pd)
 
 static void iolatency_pd_free(struct blkg_policy_data *pd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	free_percpu(iolat->stats);
 	kfree(iolat);
@@ -967,11 +1067,13 @@ static struct blkcg_policy blkcg_policy_iolatency = {
 
 static int __init iolatency_init(void)
 {
+	trace_printk("CX____ registered iolatency blkcg_policy\n\n");
 	return blkcg_policy_register(&blkcg_policy_iolatency);
 }
 
 static void __exit iolatency_exit(void)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return blkcg_policy_unregister(&blkcg_policy_iolatency);
 }
 
diff --git a/block/blk-rq-qos.c b/block/blk-rq-qos.c
index 43bcd4e7a..b121c2420 100644
--- a/block/blk-rq-qos.c
+++ b/block/blk-rq-qos.c
@@ -1,5 +1,7 @@
 #include "blk-rq-qos.h"
 
+static int g_dump_stack_counter = 0;
+
 /*
  * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
  * false if 'v' + 1 would be bigger than 'below'.
@@ -8,6 +10,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 {
 	unsigned int cur = atomic_read(v);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (;;) {
 		unsigned int old;
 
@@ -24,6 +27,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 
 bool rq_wait_inc_below(struct rq_wait *rq_wait, unsigned int limit)
 {
+	trace_printk("%s[%d] --- set rq_weit->inflight to %d\n", __FUNCTION__, __LINE__, limit);
 	return atomic_inc_below(&rq_wait->inflight, limit);
 }
 
@@ -31,6 +35,7 @@ void rq_qos_cleanup(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->cleanup)
 			rqos->ops->cleanup(rqos, bio);
@@ -41,6 +46,12 @@ void rq_qos_done(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done)
 			rqos->ops->done(rqos, rq);
@@ -51,6 +62,12 @@ void rq_qos_issue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->issue)
 			rqos->ops->issue(rqos, rq);
@@ -61,6 +78,12 @@ void rq_qos_requeue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->requeue)
 			rqos->ops->requeue(rqos, rq);
@@ -72,7 +95,13 @@ void rq_qos_throttle(struct request_queue *q, struct bio *bio,
 {
 	struct rq_qos *rqos;
 
-	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+
+	for(rqos = q->rq_qos; rqos; rqos = rqos->next) { // 可能同时有多个qos?
 		if (rqos->ops->throttle)
 			rqos->ops->throttle(rqos, bio, lock);
 	}
@@ -82,6 +111,15 @@ void rq_qos_track(struct request_queue *q, struct request *rq, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->track)
 			rqos->ops->track(rqos, rq, bio);
@@ -92,6 +130,12 @@ void rq_qos_done_bio(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done_bio)
 			rqos->ops->done_bio(rqos, bio);
@@ -106,6 +150,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 	unsigned int depth;
 	bool ret = false;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * For QD=1 devices, this is a special case. It's important for those
 	 * to have one request ready when one completes, so force a depth of
@@ -143,6 +188,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 		}
 
 		rqd->max_depth = depth;
+		trace_printk("%s[%d] ---max_depth=%d\n", __FUNCTION__, __LINE__, rqd->max_depth);
 	}
 
 	return ret;
@@ -151,6 +197,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 /* Returns true on success and false if scaling up wasn't possible */
 bool rq_depth_scale_up(struct rq_depth *rqd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Hit max in previous round, stop here
 	 */
@@ -170,6 +217,7 @@ bool rq_depth_scale_up(struct rq_depth *rqd)
  */
 bool rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Stop scaling down when we've hit the limit. This also prevents
 	 * ->scale_step from going to crazy values, if the device can't
@@ -190,6 +238,7 @@ bool rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 
 void rq_qos_exit(struct request_queue *q)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (q->rq_qos) {
 		struct rq_qos *rqos = q->rq_qos;
 		q->rq_qos = rqos->next;
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 98caba3e9..614eac174 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -74,6 +74,7 @@ static inline void rq_wait_init(struct rq_wait *rq_wait)
 
 static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rqos->next = q->rq_qos;
 	q->rq_qos = rqos;
 }
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index caee65860..211b487ec 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2145,9 +2145,12 @@ bool blk_throtl_bio(struct request_queue *q, struct blkcg_gq *blkg,
 
 	WARN_ON_ONCE(!rcu_read_lock_held());
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/* see throtl_charge_bio() */
-	if (bio_flagged(bio, BIO_THROTTLED) || !tg->has_rules[rw])
+	if (bio_flagged(bio, BIO_THROTTLED) || !tg->has_rules[rw]) {
 		goto out;
+	} else 
+		trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 
 	spin_lock_irq(q->queue_lock);
 
@@ -2210,7 +2213,8 @@ bool blk_throtl_bio(struct request_queue *q, struct blkcg_gq *blkg,
 	}
 
 	/* out-of-limit, queue to @tg */
-	throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
+	//throtl_log(sq, "[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d",
+	trace_printk("[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d\n",
 		   rw == READ ? 'R' : 'W',
 		   tg->bytes_disp[rw], bio->bi_iter.bi_size,
 		   tg_bps_limit(tg, rw),
@@ -2505,6 +2509,7 @@ static int __init throtl_init(void)
 	if (!kthrotld_workqueue)
 		panic("Failed to create kthrotld\n");
 
+	printk(KERN_ERR "CX____ registered throtl blkcg_policy\n");
 	return blkcg_policy_register(&blkcg_policy_throtl);
 }
 
diff --git a/block/blk-wbt.c b/block/blk-wbt.c
index f1de8ba48..f32bc896e 100644
--- a/block/blk-wbt.c
+++ b/block/blk-wbt.c
@@ -123,6 +123,7 @@ static void rwb_wake_all(struct rq_wb *rwb)
 	}
 }
 
+/* 将不需要wait或者不需要继续wait的队列唤醒 */
 static void wbt_rqw_done(struct rq_wb *rwb, struct rq_wait *rqw,
 			 enum wbt_flags wb_acct)
 {
@@ -157,7 +158,7 @@ static void wbt_rqw_done(struct rq_wb *rwb, struct rq_wait *rqw,
 	if (inflight && inflight >= limit)
 		return;
 
-	if (wq_has_sleeper(&rqw->wait)) {
+	if (wq_has_sleeper(&rqw->wait)) { // eturns true if the wait list is not empty
 		int diff = limit - inflight;
 
 		if (!inflight || diff >= rwb->wb_background / 2)
@@ -220,6 +221,7 @@ static u64 rwb_sync_issue_lat(struct rq_wb *rwb)
 		return 0;
 
 	now = ktime_to_ns(ktime_get());
+	trace_printk("%s[%d] --- issue_lat=%llu\n", __FUNCTION__, __LINE__, now-issue);
 	return now - issue;
 }
 
@@ -236,6 +238,9 @@ static int latency_exceeded(struct rq_wb *rwb, struct blk_rq_stat *stat)
 	struct rq_depth *rqd = &rwb->rq_depth;
 	u64 thislat;
 
+	dump_stack();
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+
 	/*
 	 * If our stored sync issue exceeds the window size, or it
 	 * exceeds our min target AND we haven't logged any entries,
@@ -303,12 +308,14 @@ static void calc_wb_limits(struct rq_wb *rwb)
 		rwb->wb_normal = (rwb->rq_depth.max_depth + 1) / 2;
 		rwb->wb_background = (rwb->rq_depth.max_depth + 3) / 4;
 	}
+	trace_printk("%s[%d] --- rwb->wb_normal=%d, rwb->wb_background=%d, rwb->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, rwb->wb_normal, rwb->wb_background, rwb->min_lat_nsec);
 }
 
 static void scale_up(struct rq_wb *rwb)
 {
 	if (!rq_depth_scale_up(&rwb->rq_depth))
 		return;
+	trace_printk("%s[%d] --- ", __FUNCTION__, __LINE__);
 	calc_wb_limits(rwb);
 	rwb->unknown_cnt = 0;
 	rwb_wake_all(rwb);
@@ -319,6 +326,7 @@ static void scale_down(struct rq_wb *rwb, bool hard_throttle)
 {
 	if (!rq_depth_scale_down(&rwb->rq_depth, hard_throttle))
 		return;
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	calc_wb_limits(rwb);
 	rwb->unknown_cnt = 0;
 	rwb_trace_step(rwb, "scale down");
@@ -328,6 +336,7 @@ static void rwb_arm_timer(struct rq_wb *rwb)
 {
 	struct rq_depth *rqd = &rwb->rq_depth;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (rqd->scale_step > 0) {
 		/*
 		 * We should speed this up, using some variant of a fast
@@ -355,6 +364,9 @@ static void wb_timer_fn(struct blk_stat_callback *cb)
 	unsigned int inflight = wbt_inflight(rwb);
 	int status;
 
+	dump_stack();
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+
 	status = latency_exceeded(rwb, cb->stat);
 
 	trace_wbt_timer(rwb->rqos.q->backing_dev_info, status, rqd->scale_step,
@@ -438,6 +450,7 @@ void wbt_set_min_lat(struct request_queue *q, u64 val)
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (!rqos)
 		return;
+	trace_printk("%s[%d] --- ", __FUNCTION__, __LINE__);
 	RQWB(rqos)->min_lat_nsec = val;
 	RQWB(rqos)->enable_state = WBT_STATE_ON_MANUAL;
 	__wbt_update_limits(RQWB(rqos));
@@ -448,6 +461,7 @@ static bool close_io(struct rq_wb *rwb)
 {
 	const unsigned long now = jiffies;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	return time_before(now, rwb->last_issue + HZ / 10) ||
 		time_before(now, rwb->last_comp + HZ / 10);
 }
@@ -502,6 +516,7 @@ struct wbt_wait_data {
 static int wbt_wake_function(struct wait_queue_entry *curr, unsigned int mode,
 			     int wake_flags, void *key)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct wbt_wait_data *data = container_of(curr, struct wbt_wait_data,
 							wq);
 
@@ -527,6 +542,7 @@ static void __wbt_wait(struct rq_wb *rwb, enum wbt_flags wb_acct,
 	__releases(lock)
 	__acquires(lock)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_wait *rqw = get_rq_wait(rwb, wb_acct);
 	struct wbt_wait_data data = {
 		.wq = {
@@ -578,6 +594,7 @@ static void __wbt_wait(struct rq_wb *rwb, enum wbt_flags wb_acct,
 
 static inline bool wbt_should_throttle(struct rq_wb *rwb, struct bio *bio)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	switch (bio_op(bio)) {
 	case REQ_OP_WRITE:
 		/*
@@ -631,6 +648,7 @@ static void wbt_wait(struct rq_qos *rqos, struct bio *bio, spinlock_t *lock)
 	struct rq_wb *rwb = RQWB(rqos);
 	enum wbt_flags flags;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	flags = bio_to_wbt_flags(rwb, bio);
 	if (!(flags & WBT_TRACKED)) {
 		if (flags & WBT_READ)
@@ -646,6 +664,7 @@ static void wbt_wait(struct rq_qos *rqos, struct bio *bio, spinlock_t *lock)
 
 static void wbt_track(struct rq_qos *rqos, struct request *rq, struct bio *bio)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_wb *rwb = RQWB(rqos);
 	rq->wbt_flags |= bio_to_wbt_flags(rwb, bio);
 }
@@ -654,6 +673,7 @@ void wbt_issue(struct rq_qos *rqos, struct request *rq)
 {
 	struct rq_wb *rwb = RQWB(rqos);
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (!rwb_enabled(rwb))
 		return;
 
@@ -683,6 +703,7 @@ void wbt_requeue(struct rq_qos *rqos, struct request *rq)
 
 void wbt_set_queue_depth(struct request_queue *q, unsigned int depth)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (rqos) {
 		RQWB(rqos)->rq_depth.queue_depth = depth;
@@ -692,6 +713,7 @@ void wbt_set_queue_depth(struct request_queue *q, unsigned int depth)
 
 void wbt_set_write_cache(struct request_queue *q, bool write_cache_on)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	if (rqos)
 		RQWB(rqos)->wc = write_cache_on;
@@ -719,6 +741,7 @@ EXPORT_SYMBOL_GPL(wbt_enable_default);
 
 u64 wbt_default_latency_nsec(struct request_queue *q)
 {
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	/*
 	 * We default to 2msec for non-rotational storage, and 75msec
 	 * for rotational storage.
@@ -733,6 +756,7 @@ static int wbt_data_dir(const struct request *rq)
 {
 	const int op = req_op(rq);
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (op == REQ_OP_READ)
 		return READ;
 	else if (op_is_write(op))
@@ -759,6 +783,7 @@ void wbt_disable_default(struct request_queue *q)
 {
 	struct rq_qos *rqos = wbt_rq_qos(q);
 	struct rq_wb *rwb;
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	if (!rqos)
 		return;
 	rwb = RQWB(rqos);
@@ -785,6 +810,7 @@ int wbt_init(struct request_queue *q)
 	struct rq_wb *rwb;
 	int i;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	rwb = kzalloc(sizeof(*rwb), GFP_KERNEL);
 	if (!rwb)
 		return -ENOMEM;
diff --git a/block/genhd.c b/block/genhd.c
index 2b2a936cf..990dae7a9 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -838,12 +838,17 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 {
 	struct gendisk *disk = NULL;
 
+	printk(KERN_ERR "get_gendisk: major=%d", MAJOR(devt));
 	if (MAJOR(devt) != BLOCK_EXT_MAJOR) {
 		struct kobject *kobj;
 
 		kobj = kobj_lookup(bdev_map, devt, partno);
 		if (kobj)
 			disk = dev_to_disk(kobj_to_dev(kobj));
+		if (!kobj)
+			printk(KERN_ERR "get_gendisk: null kobj");
+		if (!disk)
+			printk(KERN_ERR "get_gendisk: null disk");
 	} else {
 		struct hd_struct *part;
 
@@ -856,6 +861,8 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 		spin_unlock_bh(&ext_devt_lock);
 	}
 
+	if (disk)
+		printk(KERN_ERR "get_gendisk: disk name=%s", disk->disk_name);
 	if (!disk)
 		return NULL;
 
diff --git a/fs/buffer.c b/fs/buffer.c
index a550e0d8e..556b2087c 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -2986,6 +2986,7 @@ static void end_bio_bh_io_sync(struct bio *bio)
  * This allows us to do IO even on the odd last sectors
  * of a device, even if the block size is some multiple
  * of the physical sector size.
+ * 这允许我们在设备的奇数个最后扇区上执行IO偶数，即使块大小是物理扇区大小的几倍。
  *
  * We'll just truncate the bio to the size of the device,
  * and clear the end of the buffer head manually.
diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c81a1f3f0..d45592af2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2365,6 +2365,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if ((f2fs_is_atomic_file(inode) &&
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||
 			is_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		drop_atomic = true;
 		goto fail;
@@ -2388,6 +2389,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	page = f2fs_pagecache_get_page(mapping, index,
 				FGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);
 	if (!page) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		goto fail;
 	}
@@ -2455,6 +2457,7 @@ static int f2fs_write_end(struct file *file,
 {
 	struct inode *inode = page->mapping->host;
 
+    trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	/*
diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index a89e27367..2e60f1cde 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -245,6 +245,7 @@ void __inode_attach_wb(struct inode *inode, struct page *page)
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 	struct bdi_writeback *wb = NULL;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode_cgwb_enabled(inode)) {
 		struct cgroup_subsys_state *memcg_css;
 
@@ -283,6 +284,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 	__releases(&inode->i_lock)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	while (true) {
 		struct bdi_writeback *wb = inode_to_wb(inode);
 
@@ -319,6 +321,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 static struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	return locked_inode_to_wb_and_lock_list(inode);
 }
@@ -333,11 +336,13 @@ struct inode_switch_wbs_context {
 
 static void bdi_down_write_wb_switch_rwsem(struct backing_dev_info *bdi)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	down_write(&bdi->wb_switch_rwsem);
 }
 
 static void bdi_up_write_wb_switch_rwsem(struct backing_dev_info *bdi)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	up_write(&bdi->wb_switch_rwsem);
 }
 
@@ -354,6 +359,7 @@ static void inode_switch_wbs_work_fn(struct work_struct *work)
 	bool switched = false;
 	void **slot;
 
+	trace_printk("%s[%d] CG-WB --- will down_read\n", __FUNCTION__, __LINE__);
 	/*
 	 * If @inode switches cgwb membership while sync_inodes_sb() is
 	 * being issued, sync_inodes_sb() might miss it.  Synchronize.
@@ -471,6 +477,7 @@ static void inode_switch_wbs_rcu_fn(struct rcu_head *rcu_head)
 	struct inode_switch_wbs_context *isw = container_of(rcu_head,
 				struct inode_switch_wbs_context, rcu_head);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* needs to grab bh-unsafe locks, bounce to work item */
 	INIT_WORK(&isw->work, inode_switch_wbs_work_fn);
 	queue_work(isw_wq, &isw->work);
@@ -490,6 +497,7 @@ static void inode_switch_wbs(struct inode *inode, int new_wb_id)
 	struct cgroup_subsys_state *memcg_css;
 	struct inode_switch_wbs_context *isw;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* noop if seems to be already in progress */
 	if (inode->i_state & I_WB_SWITCH)
 		return;
@@ -568,6 +576,7 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 		return;
 	}
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	wbc->wb = inode_to_wb(inode);
 	wbc->inode = inode;
 
@@ -637,6 +646,7 @@ void wbc_detach_inode(struct writeback_control *wbc)
 	u16 history;
 	int max_id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!wb)
 		return;
 
@@ -727,6 +737,7 @@ void wbc_account_io(struct writeback_control *wbc, struct page *page,
 	struct cgroup_subsys_state *css;
 	int id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
@@ -779,6 +790,8 @@ EXPORT_SYMBOL_GPL(wbc_account_io);
  */
 int inode_congested(struct inode *inode, int cong_bits)
 {
+	static int cnt = 0;
+
 	/*
 	 * Once set, ->i_wb never becomes NULL while the inode is alive.
 	 * Start transaction iff ->i_wb is visible.
@@ -790,6 +803,11 @@ int inode_congested(struct inode *inode, int cong_bits)
 
 		wb = unlocked_inode_to_wb_begin(inode, &lock_cookie);
 		congested = wb_congested(wb, cong_bits);
+		if (cnt > 200) {
+			trace_printk("%s[%d] CG-WB --- congested=%d\n", __FUNCTION__, __LINE__, congested);
+			cnt = 0;
+		} else 
+			cnt ++;
 		unlocked_inode_to_wb_end(inode, &lock_cookie);
 		return congested;
 	}
@@ -812,6 +830,7 @@ static long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)
 	unsigned long this_bw = wb->avg_write_bandwidth;
 	unsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (nr_pages == LONG_MAX)
 		return LONG_MAX;
 
@@ -845,6 +864,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
 	struct bdi_writeback *wb = list_entry(&bdi->wb_list,
 					      struct bdi_writeback, bdi_node);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	might_sleep();
 restart:
 	rcu_read_lock();
@@ -917,6 +937,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
  */
 void cgroup_writeback_umount(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (atomic_read(&isw_nr_in_flight)) {
 		/*
 		 * Use rcu_barrier() to wait for all pending callbacks to
@@ -929,6 +950,7 @@ void cgroup_writeback_umount(void)
 
 static int __init cgroup_writeback_init(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	isw_wq = alloc_workqueue("inode_switch_wbs", 0, 0);
 	if (!isw_wq)
 		return -ENOMEM;
@@ -1557,6 +1579,7 @@ static long writeback_sb_inodes(struct super_block *sb,
 	long write_chunk;
 	long wrote = 0;  /* count both pages and inodes */
 
+	trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (!list_empty(&wb->b_io)) {
 		struct inode *inode = wb_inode(wb->b_io.prev);
 		struct bdi_writeback *tmp_wb;
@@ -1925,6 +1948,7 @@ static long wb_check_start_all(struct bdi_writeback *wb)
 		return 0;
 
 	nr_pages = get_nr_dirty_pages();
+	trace_printk("CX____ wb nr_pages=%ld\n", nr_pages);
 	if (nr_pages) {
 		struct wb_writeback_work work = {
 			.nr_pages	= wb_split_bdi_pages(wb, nr_pages),
@@ -1951,6 +1975,7 @@ static long wb_do_writeback(struct bdi_writeback *wb)
 
 	set_bit(WB_writeback_running, &wb->state);
 	while ((work = get_next_work_item(wb)) != NULL) {
+		trace_printk("CX____ wb do next work, for_forground=%d, kupdate=%d\n", work->for_background, work->for_kupdate);
 		trace_writeback_exec(wb, work);
 		wrote += wb_writeback(wb, work);
 		finish_writeback_work(wb, work);
diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 07e02d6df..3942ea8d3 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -238,6 +238,7 @@ struct wb_lock_cookie {
  */
 static inline bool wb_tryget(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		return percpu_ref_tryget(&wb->refcnt);
 	return true;
@@ -249,6 +250,7 @@ static inline bool wb_tryget(struct bdi_writeback *wb)
  */
 static inline void wb_get(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		percpu_ref_get(&wb->refcnt);
 }
@@ -259,6 +261,7 @@ static inline void wb_get(struct bdi_writeback *wb)
  */
 static inline void wb_put(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (WARN_ON_ONCE(!wb->bdi)) {
 		/*
 		 * A driver bug might cause a file to be removed before bdi was
@@ -279,6 +282,7 @@ static inline void wb_put(struct bdi_writeback *wb)
  */
 static inline bool wb_dying(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return percpu_ref_is_dying(&wb->refcnt);
 }
 
diff --git a/include/linux/backing-dev.h b/include/linux/backing-dev.h
index c28a47cbe..cd78ccd2a 100644
--- a/include/linux/backing-dev.h
+++ b/include/linux/backing-dev.h
@@ -248,6 +248,7 @@ static inline bool inode_cgwb_enabled(struct inode *inode)
 {
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return cgroup_subsys_on_dfl(memory_cgrp_subsys) &&
 		cgroup_subsys_on_dfl(io_cgrp_subsys) &&
 		bdi_cap_account_dirty(bdi) &&
@@ -268,6 +269,7 @@ static inline struct bdi_writeback *wb_find_current(struct backing_dev_info *bdi
 	struct cgroup_subsys_state *memcg_css;
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	memcg_css = task_css(current, memory_cgrp_id);
 	if (!memcg_css->parent)
 		return &bdi->wb;
@@ -297,6 +299,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
 {
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	wb = wb_find_current(bdi);
 	if (wb && unlikely(!wb_tryget(wb)))
@@ -322,6 +325,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
  */
 static inline bool inode_to_wb_is_valid(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return inode->i_wb;
 }
 
@@ -335,6 +339,7 @@ static inline bool inode_to_wb_is_valid(struct inode *inode)
  */
 static inline struct bdi_writeback *inode_to_wb(const struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_LOCKDEP
 	WARN_ON_ONCE(debug_locks &&
 		     (!lockdep_is_held(&inode->i_lock) &&
@@ -364,6 +369,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 {
 	rcu_read_lock();
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Paired with store_release in inode_switch_wb_work_fn() and
 	 * ensures that we see the new wb if we see cleared I_WB_SWITCH.
@@ -388,6 +394,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 static inline void unlocked_inode_to_wb_end(struct inode *inode,
 					    struct wb_lock_cookie *cookie)
 {
+	//trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (unlikely(cookie->locked))
 		xa_unlock_irqrestore(&inode->i_mapping->i_pages, cookie->flags);
 
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index 6d766a19f..d25182279 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -253,6 +253,8 @@ static inline bool blk_cgroup_congested(void)
 	struct cgroup_subsys_state *css;
 	bool ret = false;
 
+	//CX____
+	//dump_stack();
 	rcu_read_lock();
 	css = kthread_blkcg();
 	if (!css)
@@ -399,6 +401,7 @@ extern void blkcg_destroy_blkgs(struct blkcg *blkcg);
  */
 static inline void blkcg_cgwb_get(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	refcount_inc(&blkcg->cgwb_refcnt);
 }
 
@@ -414,6 +417,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg)
  */
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (refcount_dec_and_test(&blkcg->cgwb_refcnt))
 		blkcg_destroy_blkgs(blkcg);
 }
@@ -424,6 +428,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg) { }
 
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] no CG-WB --- !!!!!!\n", __FUNCTION__, __LINE__);
 	/* wb isn't being accounted, so trigger destruction right away */
 	blkcg_destroy_blkgs(blkcg);
 }
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index f6dfb3073..089773244 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -135,6 +135,8 @@ static inline void bio_issue_init(struct bio_issue *issue,
 	issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
 			(ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
 			((u64)size << BIO_ISSUE_SIZE_SHIFT));
+    //printk(KERN_ERR "CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
+    //trace_printk("CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
 }
 
 /*
diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e34..bba7029e0 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -203,6 +203,7 @@ void cgroup_writeback_umount(void);
  */
 static inline void inode_attach_wb(struct inode *inode, struct page *page)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!inode->i_wb)
 		__inode_attach_wb(inode, page);
 }
@@ -215,6 +216,7 @@ static inline void inode_attach_wb(struct inode *inode, struct page *page)
  */
 static inline void inode_detach_wb(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode->i_wb) {
 		WARN_ON_ONCE(!(inode->i_state & I_CLEAR));
 		wb_put(inode->i_wb);
@@ -234,6 +236,7 @@ static inline void inode_detach_wb(struct inode *inode)
 static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
 					       struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	inode_attach_wb(inode, NULL);
 	wbc_attach_and_unlock_inode(wbc, inode);
@@ -250,6 +253,7 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
diff --git a/include/trace/events/writeback.h b/include/trace/events/writeback.h
index 32db72c7c..748da2e1c 100644
--- a/include/trace/events/writeback.h
+++ b/include/trace/events/writeback.h
@@ -137,11 +137,13 @@ DEFINE_EVENT(writeback_dirty_inode_template, writeback_dirty_inode,
 
 static inline unsigned int __trace_wb_assign_cgroup(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return wb->memcg_css->cgroup->kn->id.ino;
 }
 
 static inline unsigned int __trace_wbc_assign_cgroup(struct writeback_control *wbc)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wbc->wb)
 		return __trace_wb_assign_cgroup(wbc->wb);
 	else
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index 78ef274b0..e99dcd6f3 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -2663,6 +2663,7 @@ struct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)
 	if (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)
 		return ERR_PTR(-EINVAL);
 
+    trace_printk("CX____ %s[%d]: pid=%d\n", __FUNCTION__, __LINE__, pid);
 	percpu_down_write(&cgroup_threadgroup_rwsem);
 
 	rcu_read_lock();
