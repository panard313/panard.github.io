diff --git a/Makefile b/Makefile
index f1e428271..962d31505 100644
--- a/Makefile
+++ b/Makefile
@@ -662,7 +662,7 @@ else
 ifdef CONFIG_PROFILE_ALL_BRANCHES
 KBUILD_CFLAGS	+= -O2 $(call cc-disable-warning,maybe-uninitialized,)
 else
-KBUILD_CFLAGS   += -O2
+KBUILD_CFLAGS   += -O1
 endif
 endif
 
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index a06547fe6..da5bb42bc 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -821,6 +821,7 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	int key_len, part, ret;
 	char *body;
 
+	printk(KERN_ERR "blkg_conf_prep %s", input);
 	if (sscanf(input, "%u:%u%n", &major, &minor, &key_len) != 2)
 		return -EINVAL;
 
@@ -830,8 +831,11 @@ int blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,
 	body = skip_spaces(body);
 
 	disk = get_gendisk(MKDEV(major, minor), &part);
-	if (!disk)
+	if (!disk) {
+		printk(KERN_ERR "blkg_conf_prep null diks");
 		return -ENODEV;
+	}
+	printk(KERN_ERR "blkg_conf_prep part=%d", part);
 	if (part) {
 		ret = -ENODEV;
 		goto fail;
@@ -1682,6 +1686,7 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	u64 delay_nsec = 0;
 	int tok;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	while (blkg->parent) {
 		if (atomic_read(&blkg->use_delay)) {
 			blkcg_scale_delay(blkg, now);
@@ -1709,6 +1714,8 @@ static void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)
 	 * to do a psi_memstall_enter/leave if memdelay is set.
 	 */
 
+	trace_printk("%s[%d] --- delay_nsec=%lly\n", __FUNCTION__, __LINE__, delay_nsec);
+
 	exp = ktime_add_ns(now, delay_nsec);
 	tok = io_schedule_prepare();
 	do {
@@ -1740,6 +1747,7 @@ void blkcg_maybe_throttle_current(void)
 	if (!q)
 		return;
 
+	trace_printk("%s[%d] --- \n", __FUNCTION__, __LINE__);
 	current->throttle_queue = NULL;
 	current->use_memdelay = false;
 
@@ -1795,8 +1803,10 @@ void blkcg_schedule_throttle(struct request_queue *q, bool use_memdelay)
 	if (!blk_get_queue(q))
 		return;
 
-	if (current->throttle_queue)
+	if (current->throttle_queue) {
+		trace_printk("%s[%d] --- current->throttle_queue.id=%d\n", __FUNCTION__, __LINE__, current->throttle_queue.id);
 		blk_put_queue(current->throttle_queue);
+	}
 	current->throttle_queue = q;
 	if (use_memdelay)
 		current->use_memdelay = use_memdelay;
@@ -1816,6 +1826,7 @@ void blkcg_add_delay(struct blkcg_gq *blkg, u64 now, u64 delta)
 {
 	blkcg_scale_delay(blkg, now);
 	atomic64_add(delta, &blkg->delay_nsec);
+	trace_printk("CX____ %s[%d] add %llu (ns), to %llu (ns)\n", __FUNCTION__, __LINE__, delta, blkg->delay_nsec);
 }
 EXPORT_SYMBOL_GPL(blkcg_add_delay);
 
diff --git a/block/blk-core.c b/block/blk-core.c
index ea33d6abd..fd0d9fb5a 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -145,7 +145,9 @@ EXPORT_SYMBOL_GPL(blk_queue_flag_test_and_clear);
 
 static void blk_clear_congested(struct request_list *rl, int sync)
 {
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	clear_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/*
@@ -159,7 +161,9 @@ static void blk_clear_congested(struct request_list *rl, int sync)
 
 static void blk_set_congested(struct request_list *rl, int sync)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_CGROUP_WRITEBACK
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	set_wb_congested(rl->blkg->wb_congested, sync);
 #else
 	/* see blk_clear_congested() */
@@ -172,15 +176,18 @@ void blk_queue_congestion_threshold(struct request_queue *q)
 {
 	int nr;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	nr = q->nr_requests - (q->nr_requests / 8) + 1;
 	if (nr > q->nr_requests)
 		nr = q->nr_requests;
 	q->nr_congestion_on = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_on=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_on);
 
 	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
 	if (nr < 1)
 		nr = 1;
 	q->nr_congestion_off = nr;
+	trace_printk("%s[%d] --- q->nr_congestion_off=%d\n", __FUNCTION__, __LINE__, q->nr_congestion_off);
 }
 
 void blk_rq_init(struct request_queue *q, struct request *rq)
diff --git a/block/blk-iolatency.c b/block/blk-iolatency.c
index 0529e94a2..460ba26cb 100644
--- a/block/blk-iolatency.c
+++ b/block/blk-iolatency.c
@@ -79,6 +79,8 @@
 
 #define DEFAULT_SCALE_COOKIE 1000000U
 
+static int g_dump_stack_counter = 0;
+
 static struct blkcg_policy blkcg_policy_iolatency;
 struct iolatency_grp;
 
@@ -90,11 +92,13 @@ struct blk_iolatency {
 
 static inline struct blk_iolatency *BLKIOLATENCY(struct rq_qos *rqos)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return container_of(rqos, struct blk_iolatency, rqos);
 }
 
 static inline bool blk_iolatency_enabled(struct blk_iolatency *blkiolat)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return atomic_read(&blkiolat->enabled) > 0;
 }
 
@@ -180,6 +184,7 @@ static inline bool iolatency_may_queue(struct iolatency_grp *iolat,
 {
 	struct rq_wait *rqw = &iolat->rq_wait;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (first_block && waitqueue_active(&rqw->wait) &&
 	    rqw->wait.head.next != &wait->entry)
 		return false;
@@ -198,6 +203,7 @@ static void __blkcg_iolatency_throttle(struct rq_qos *rqos,
 	DEFINE_WAIT(wait);
 	bool first_block = true;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (use_delay)
 		blkcg_schedule_throttle(rqos->q, use_memdelay);
 
@@ -263,6 +269,7 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 	unsigned long max_scale = qd << 1;
 	unsigned long diff = 0;
 
+	trace_printk("%s[%d] --- entry, cookie=%d, up=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter, up);
 	if (old < DEFAULT_SCALE_COOKIE)
 		diff = DEFAULT_SCALE_COOKIE - old;
 
@@ -288,6 +295,7 @@ static void scale_cookie_change(struct blk_iolatency *blkiolat,
 			atomic_sub(scale, &lat_info->scale_cookie);
 		}
 	}
+	trace_printk("%s[%d] --- finish, cookie=%d\n", __FUNCTION__, __LINE__, lat_info->scale_cookie.counter);
 }
 
 /*
@@ -305,6 +313,7 @@ static void scale_change(struct iolatency_grp *iolat, bool up)
 	if (old > qd)
 		old = qd;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (up) {
 		if (old == 1 && blkcg_unuse_delay(lat_to_blkg(iolat)))
 			return;
@@ -334,6 +343,7 @@ static void check_scale_change(struct iolatency_grp *iolat)
 	unsigned int old;
 	int direction = 0;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (lat_to_blkg(iolat)->parent == NULL)
 		return;
 
@@ -402,6 +412,14 @@ static void blkcg_iolatency_throttle(struct rq_qos *rqos, struct bio *bio,
 	struct request_queue *q = rqos->q;
 	bool issue_as_root = bio_issue_as_root_blkg(bio);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	if (!blk_iolatency_enabled(blkiolat))
 		return;
 
@@ -449,6 +467,7 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 	u64 start = bio_issue_time(issue);
 	u64 req_time;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Have to do this so we are truncated to the correct time that our
 	 * issue is truncated to.
@@ -468,11 +487,13 @@ static void iolatency_record_time(struct iolatency_grp *iolat,
 		u64 sub = iolat->min_lat_nsec;
 		if (req_time < sub)
 			blkcg_add_delay(lat_to_blkg(iolat), now, sub - req_time);
+			//trace_printk("%s[%d] --- root grp, add delay\n", __FUNCTION__, __LINE__);
 		return;
 	}
 
 	rq_stat = get_cpu_ptr(iolat->stats);
 	blk_rq_stat_add(rq_stat, req_time);
+	trace_printk("%s[%d] --- blk_rq_stat_add=%llu\n", __FUNCTION__, __LINE__, rq_stat);
 	put_cpu_ptr(rq_stat);
 }
 
@@ -488,6 +509,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	unsigned long flags;
 	int cpu, exp_idx;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blk_rq_stat_init(&stat);
 	preempt_disable();
 	for_each_online_cpu(cpu) {
@@ -499,10 +521,14 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	preempt_enable();
 
 	parent = blkg_to_lat(blkg->parent);
-	if (!parent)
+	if (!parent) {
+		trace_printk("%s[%d] --- ignore root grp?\n", __FUNCTION__, __LINE__);
 		return;
+	}
 
 	lat_info = &parent->child_lat;
+	trace_printk("CX____ %s[%d]: lat_info->scale_lat=%lld\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
+	//printk(KERN_ERR "CX____ %s[%d]: lat_info->scale_lat=%lld", __FUNCTION__, __LINE__, lat_info->scale_lat);
 
 	/*
 	 * CALC_LOAD takes in a number stored in fixed point representation.
@@ -527,10 +553,13 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 	lat_info->nr_samples += stat.nr_samples;
 	iolat->nr_samples = stat.nr_samples;
 
+    trace_printk("%s[%d]: scale_lat=%llu\n", __FUNCTION__, __LINE__, lat_info->scale_lat);
 	if ((lat_info->last_scale_event >= now ||
 	    now - lat_info->last_scale_event < BLKIOLATENCY_MIN_ADJUST_TIME) &&
-	    lat_info->scale_lat <= iolat->min_lat_nsec)
+	    lat_info->scale_lat <= iolat->min_lat_nsec){
+        trace_printk("%s[%d]: no need, goto out\n", __FUNCTION__, __LINE__);
 		goto out;
+	}
 
 	if (stat.mean <= iolat->min_lat_nsec &&
 	    stat.nr_samples >= BLKIOLATENCY_MIN_GOOD_SAMPLES) {
@@ -545,6 +574,7 @@ static void iolatency_check_latencies(struct iolatency_grp *iolat, u64 now)
 			WRITE_ONCE(lat_info->scale_lat, iolat->min_lat_nsec);
 			lat_info->scale_grp = iolat;
 		}
+		trace_printk("%s[%d] --- stat.mean=%llu > iolat->min_lat_nsec=%llu\n", __FUNCTION__, __LINE__, stat.mean, iolat->min_lat_nsec);
 		scale_cookie_change(iolat->blkiolat, lat_info, false);
 	}
 out:
@@ -562,11 +592,15 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 	bool enabled = false;
 	int inflight = 0;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkg = bio->bi_blkg;
 	if (!blkg)
 		return;
 
 	iolat = blkg_to_lat(bio->bi_blkg);
+	if (!issue_as_root) {
+		trace_printk("CX____ %s[%d]\n", __FUNCTION__, __LINE__);
+	}
 	if (!iolat)
 		return;
 
@@ -583,6 +617,7 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 		rqw = &iolat->rq_wait;
 
 		inflight = atomic_dec_return(&rqw->inflight);
+		trace_printk("CX____ %s[%d] inflight=%d\n", __FUNCTION__, __LINE__, inflight);
 		WARN_ON_ONCE(inflight < 0);
 		/*
 		 * If bi_status is BLK_STS_AGAIN, the bio wasn't actually
@@ -591,11 +626,13 @@ static void blkcg_iolatency_done_bio(struct rq_qos *rqos, struct bio *bio)
 		if (iolat->min_lat_nsec && bio->bi_status != BLK_STS_AGAIN) {
 			iolatency_record_time(iolat, &bio->bi_issue, now,
 					      issue_as_root);
+			trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 			window_start = atomic64_read(&iolat->window_start);
 			if (now > window_start &&
 			    (now - window_start) >= iolat->cur_win_nsec) {
 				if (atomic64_cmpxchg(&iolat->window_start,
 					     window_start, now) == window_start)
+					trace_printk("CX____ %s[%d] --- over window? will check latency  now-window_start=%llu\n", __FUNCTION__, __LINE__, now-window_start);
 					iolatency_check_latencies(iolat, now);
 			}
 		}
@@ -608,6 +645,7 @@ static void blkcg_iolatency_exit(struct rq_qos *rqos)
 {
 	struct blk_iolatency *blkiolat = BLKIOLATENCY(rqos);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	del_timer_sync(&blkiolat->timer);
 	blkcg_deactivate_policy(rqos->q, &blkcg_policy_iolatency);
 	kfree(blkiolat);
@@ -626,6 +664,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 	struct cgroup_subsys_state *pos_css;
 	u64 now = ktime_to_ns(ktime_get());
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	blkg_for_each_descendant_pre(blkg, pos_css,
 				     blkiolat->rqos.q->root_blkg) {
@@ -660,6 +699,7 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * on.
 		 */
 		if (lat_info->scale_grp == NULL) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			scale_cookie_change(iolat->blkiolat, lat_info, true);
 			goto next_lock;
 		}
@@ -670,11 +710,15 @@ static void blkiolatency_timer_fn(struct timer_list *t)
 		 * doing any IO currently.
 		 */
 		if (now - lat_info->last_scale_event >=
-		    ((u64)NSEC_PER_SEC * 5))
+		    ((u64)NSEC_PER_SEC * 5)) {
+			trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 			lat_info->scale_grp = NULL;
+		}
 next_lock:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		spin_unlock_irqrestore(&lat_info->lock, flags);
 next:
+		trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 		blkg_put(blkg);
 	}
 	rcu_read_unlock();
@@ -686,6 +730,7 @@ int blk_iolatency_init(struct request_queue *q)
 	struct rq_qos *rqos;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkiolat = kzalloc(sizeof(*blkiolat), GFP_KERNEL);
 	if (!blkiolat)
 		return -ENOMEM;
@@ -718,10 +763,13 @@ static int iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 	struct iolatency_grp *iolat = blkg_to_lat(blkg);
 	u64 oldval = iolat->min_lat_nsec;
 
+	trace_printk("%s[%d] --- val=%llu\n", __FUNCTION__, __LINE__, val);
+	printk(KERN_ERR "%s[%d] --- val=%llu", __FUNCTION__, __LINE__, val);
 	iolat->min_lat_nsec = val;
 	iolat->cur_win_nsec = max_t(u64, val << 4, BLKIOLATENCY_MIN_WIN_SIZE);
 	iolat->cur_win_nsec = min_t(u64, iolat->cur_win_nsec,
 				    BLKIOLATENCY_MAX_WIN_SIZE);
+	printk(KERN_ERR "%s[%d] --- cur_win_nsec=%llu", __FUNCTION__, __LINE__, iolat->cur_win_nsec);
 
 	if (!oldval && val)
 		return 1;
@@ -734,6 +782,7 @@ static int iolatency_set_min_lat_nsec(struct blkcg_gq *blkg, u64 val)
 
 static void iolatency_clear_scaling(struct blkcg_gq *blkg)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (blkg->parent) {
 		struct iolatency_grp *iolat = blkg_to_lat(blkg->parent);
 		struct child_latency_info *lat_info;
@@ -764,6 +813,7 @@ static ssize_t iolatency_set_limit(struct kernfs_open_file *of, char *buf,
 	int ret;
 	int enable = 0;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	ret = blkg_conf_prep(blkcg, &blkcg_policy_iolatency, buf, &ctx);
 	if (ret)
 		return ret;
@@ -838,6 +888,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	const char *dname = blkg_dev_name(pd->blkg);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (!dname || !iolat->min_lat_nsec)
 		return 0;
 	seq_printf(sf, "%s target=%llu\n",
@@ -847,6 +898,7 @@ static u64 iolatency_prfill_limit(struct seq_file *sf,
 
 static int iolatency_print_limit(struct seq_file *sf, void *v)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	blkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),
 			  iolatency_prfill_limit,
 			  &blkcg_policy_iolatency, seq_cft(sf)->private, false);
@@ -860,6 +912,7 @@ static size_t iolatency_pd_stat(struct blkg_policy_data *pd, char *buf,
 	unsigned long long avg_lat = div64_u64(iolat->lat_avg, NSEC_PER_USEC);
 	unsigned long long cur_win = div64_u64(iolat->cur_win_nsec, NSEC_PER_MSEC);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	if (iolat->rq_depth.max_depth == UINT_MAX)
 		return scnprintf(buf, size, " depth=max avg_lat=%llu win=%llu",
 				 avg_lat, cur_win);
@@ -873,6 +926,7 @@ static struct blkg_policy_data *iolatency_pd_alloc(gfp_t gfp, int node)
 {
 	struct iolatency_grp *iolat;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	iolat = kzalloc_node(sizeof(*iolat), gfp, node);
 	if (!iolat)
 		return NULL;
@@ -894,6 +948,7 @@ static void iolatency_pd_init(struct blkg_policy_data *pd)
 	u64 now = ktime_to_ns(ktime_get());
 	int cpu;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		struct blk_rq_stat *stat;
 		stat = per_cpu_ptr(iolat->stats, cpu);
@@ -931,6 +986,7 @@ static void iolatency_pd_offline(struct blkg_policy_data *pd)
 	struct blk_iolatency *blkiolat = iolat->blkiolat;
 	int ret;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	ret = iolatency_set_min_lat_nsec(blkg, 0);
 	if (ret == 1)
 		atomic_inc(&blkiolat->enabled);
@@ -941,6 +997,7 @@ static void iolatency_pd_offline(struct blkg_policy_data *pd)
 
 static void iolatency_pd_free(struct blkg_policy_data *pd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	struct iolatency_grp *iolat = pd_to_lat(pd);
 	free_percpu(iolat->stats);
 	kfree(iolat);
@@ -967,11 +1024,13 @@ static struct blkcg_policy blkcg_policy_iolatency = {
 
 static int __init iolatency_init(void)
 {
+	trace_printk("CX____ registered iolatency blkcg_policy\n\n");
 	return blkcg_policy_register(&blkcg_policy_iolatency);
 }
 
 static void __exit iolatency_exit(void)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return blkcg_policy_unregister(&blkcg_policy_iolatency);
 }
 
diff --git a/block/blk-rq-qos.c b/block/blk-rq-qos.c
index 43bcd4e7a..a7c94e29f 100644
--- a/block/blk-rq-qos.c
+++ b/block/blk-rq-qos.c
@@ -1,5 +1,7 @@
 #include "blk-rq-qos.h"
 
+static int g_dump_stack_counter = 0;
+
 /*
  * Increment 'v', if 'v' is below 'below'. Returns true if we succeeded,
  * false if 'v' + 1 would be bigger than 'below'.
@@ -8,6 +10,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 {
 	unsigned int cur = atomic_read(v);
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (;;) {
 		unsigned int old;
 
@@ -24,6 +27,7 @@ static bool atomic_inc_below(atomic_t *v, unsigned int below)
 
 bool rq_wait_inc_below(struct rq_wait *rq_wait, unsigned int limit)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	return atomic_inc_below(&rq_wait->inflight, limit);
 }
 
@@ -31,6 +35,7 @@ void rq_qos_cleanup(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->cleanup)
 			rqos->ops->cleanup(rqos, bio);
@@ -41,6 +46,12 @@ void rq_qos_done(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for (rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done)
 			rqos->ops->done(rqos, rq);
@@ -51,6 +62,12 @@ void rq_qos_issue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->issue)
 			rqos->ops->issue(rqos, rq);
@@ -61,6 +78,12 @@ void rq_qos_requeue(struct request_queue *q, struct request *rq)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->requeue)
 			rqos->ops->requeue(rqos, rq);
@@ -72,7 +95,13 @@ void rq_qos_throttle(struct request_queue *q, struct bio *bio,
 {
 	struct rq_qos *rqos;
 
-	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+
+	for(rqos = q->rq_qos; rqos; rqos = rqos->next) { // 可能同时有多个qos?
 		if (rqos->ops->throttle)
 			rqos->ops->throttle(rqos, bio, lock);
 	}
@@ -82,6 +111,15 @@ void rq_qos_track(struct request_queue *q, struct request *rq, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	//trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__); //empty in iolatency
+
+	if (g_dump_stack_counter > 100) {
+		//dump_stack();
+		g_dump_stack_counter = 0;
+	} else {
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->track)
 			rqos->ops->track(rqos, rq, bio);
@@ -92,6 +130,12 @@ void rq_qos_done_bio(struct request_queue *q, struct bio *bio)
 {
 	struct rq_qos *rqos;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
+	if (g_dump_stack_counter%11 == 0){
+		//dump_stack();
+		g_dump_stack_counter++;
+	}
+
 	for(rqos = q->rq_qos; rqos; rqos = rqos->next) {
 		if (rqos->ops->done_bio)
 			rqos->ops->done_bio(rqos, bio);
@@ -106,6 +150,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 	unsigned int depth;
 	bool ret = false;
 
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * For QD=1 devices, this is a special case. It's important for those
 	 * to have one request ready when one completes, so force a depth of
@@ -151,6 +196,7 @@ bool rq_depth_calc_max_depth(struct rq_depth *rqd)
 /* Returns true on success and false if scaling up wasn't possible */
 bool rq_depth_scale_up(struct rq_depth *rqd)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Hit max in previous round, stop here
 	 */
@@ -170,6 +216,7 @@ bool rq_depth_scale_up(struct rq_depth *rqd)
  */
 bool rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Stop scaling down when we've hit the limit. This also prevents
 	 * ->scale_step from going to crazy values, if the device can't
@@ -190,6 +237,7 @@ bool rq_depth_scale_down(struct rq_depth *rqd, bool hard_throttle)
 
 void rq_qos_exit(struct request_queue *q)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (q->rq_qos) {
 		struct rq_qos *rqos = q->rq_qos;
 		q->rq_qos = rqos->next;
diff --git a/block/blk-rq-qos.h b/block/blk-rq-qos.h
index 98caba3e9..614eac174 100644
--- a/block/blk-rq-qos.h
+++ b/block/blk-rq-qos.h
@@ -74,6 +74,7 @@ static inline void rq_wait_init(struct rq_wait *rq_wait)
 
 static inline void rq_qos_add(struct request_queue *q, struct rq_qos *rqos)
 {
+	trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	rqos->next = q->rq_qos;
 	q->rq_qos = rqos;
 }
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index caee65860..7376a65a5 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -2505,6 +2505,7 @@ static int __init throtl_init(void)
 	if (!kthrotld_workqueue)
 		panic("Failed to create kthrotld\n");
 
+	printk(KERN_ERR "CX____ registered throtl blkcg_policy\n");
 	return blkcg_policy_register(&blkcg_policy_throtl);
 }
 
diff --git a/block/genhd.c b/block/genhd.c
index 2b2a936cf..990dae7a9 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -838,12 +838,17 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 {
 	struct gendisk *disk = NULL;
 
+	printk(KERN_ERR "get_gendisk: major=%d", MAJOR(devt));
 	if (MAJOR(devt) != BLOCK_EXT_MAJOR) {
 		struct kobject *kobj;
 
 		kobj = kobj_lookup(bdev_map, devt, partno);
 		if (kobj)
 			disk = dev_to_disk(kobj_to_dev(kobj));
+		if (!kobj)
+			printk(KERN_ERR "get_gendisk: null kobj");
+		if (!disk)
+			printk(KERN_ERR "get_gendisk: null disk");
 	} else {
 		struct hd_struct *part;
 
@@ -856,6 +861,8 @@ struct gendisk *get_gendisk(dev_t devt, int *partno)
 		spin_unlock_bh(&ext_devt_lock);
 	}
 
+	if (disk)
+		printk(KERN_ERR "get_gendisk: disk name=%s", disk->disk_name);
 	if (!disk)
 		return NULL;
 
diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
index c81a1f3f0..d45592af2 100644
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -2365,6 +2365,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	if ((f2fs_is_atomic_file(inode) &&
 			!f2fs_available_free_memory(sbi, INMEM_PAGES)) ||
 			is_inode_flag_set(inode, FI_ATOMIC_REVOKE_REQUEST)) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		drop_atomic = true;
 		goto fail;
@@ -2388,6 +2389,7 @@ static int f2fs_write_begin(struct file *file, struct address_space *mapping,
 	page = f2fs_pagecache_get_page(mapping, index,
 				FGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);
 	if (!page) {
+        trace_printk("%s[%d]: no memory\n", __FUNCTION__, __LINE__);
 		err = -ENOMEM;
 		goto fail;
 	}
@@ -2455,6 +2457,7 @@ static int f2fs_write_end(struct file *file,
 {
 	struct inode *inode = page->mapping->host;
 
+    trace_printk("%s[%d] ---\n", __FUNCTION__, __LINE__);
 	trace_f2fs_write_end(inode, pos, len, copied);
 
 	/*
diff --git a/fs/fs-writeback.c b/fs/fs-writeback.c
index a89e27367..af2867962 100644
--- a/fs/fs-writeback.c
+++ b/fs/fs-writeback.c
@@ -245,6 +245,7 @@ void __inode_attach_wb(struct inode *inode, struct page *page)
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 	struct bdi_writeback *wb = NULL;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode_cgwb_enabled(inode)) {
 		struct cgroup_subsys_state *memcg_css;
 
@@ -283,6 +284,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 	__releases(&inode->i_lock)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	while (true) {
 		struct bdi_writeback *wb = inode_to_wb(inode);
 
@@ -319,6 +321,7 @@ locked_inode_to_wb_and_lock_list(struct inode *inode)
 static struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)
 	__acquires(&wb->list_lock)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	return locked_inode_to_wb_and_lock_list(inode);
 }
@@ -333,11 +336,13 @@ struct inode_switch_wbs_context {
 
 static void bdi_down_write_wb_switch_rwsem(struct backing_dev_info *bdi)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	down_write(&bdi->wb_switch_rwsem);
 }
 
 static void bdi_up_write_wb_switch_rwsem(struct backing_dev_info *bdi)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	up_write(&bdi->wb_switch_rwsem);
 }
 
@@ -354,6 +359,7 @@ static void inode_switch_wbs_work_fn(struct work_struct *work)
 	bool switched = false;
 	void **slot;
 
+	trace_printk("%s[%d] CG-WB --- will down_read\n", __FUNCTION__, __LINE__);
 	/*
 	 * If @inode switches cgwb membership while sync_inodes_sb() is
 	 * being issued, sync_inodes_sb() might miss it.  Synchronize.
@@ -471,6 +477,7 @@ static void inode_switch_wbs_rcu_fn(struct rcu_head *rcu_head)
 	struct inode_switch_wbs_context *isw = container_of(rcu_head,
 				struct inode_switch_wbs_context, rcu_head);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* needs to grab bh-unsafe locks, bounce to work item */
 	INIT_WORK(&isw->work, inode_switch_wbs_work_fn);
 	queue_work(isw_wq, &isw->work);
@@ -490,6 +497,7 @@ static void inode_switch_wbs(struct inode *inode, int new_wb_id)
 	struct cgroup_subsys_state *memcg_css;
 	struct inode_switch_wbs_context *isw;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/* noop if seems to be already in progress */
 	if (inode->i_state & I_WB_SWITCH)
 		return;
@@ -568,6 +576,7 @@ void wbc_attach_and_unlock_inode(struct writeback_control *wbc,
 		return;
 	}
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	wbc->wb = inode_to_wb(inode);
 	wbc->inode = inode;
 
@@ -637,6 +646,7 @@ void wbc_detach_inode(struct writeback_control *wbc)
 	u16 history;
 	int max_id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!wb)
 		return;
 
@@ -727,6 +737,7 @@ void wbc_account_io(struct writeback_control *wbc, struct page *page,
 	struct cgroup_subsys_state *css;
 	int id;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
@@ -779,6 +790,7 @@ EXPORT_SYMBOL_GPL(wbc_account_io);
  */
 int inode_congested(struct inode *inode, int cong_bits)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Once set, ->i_wb never becomes NULL while the inode is alive.
 	 * Start transaction iff ->i_wb is visible.
@@ -812,6 +824,7 @@ static long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)
 	unsigned long this_bw = wb->avg_write_bandwidth;
 	unsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (nr_pages == LONG_MAX)
 		return LONG_MAX;
 
@@ -845,6 +858,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
 	struct bdi_writeback *wb = list_entry(&bdi->wb_list,
 					      struct bdi_writeback, bdi_node);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	might_sleep();
 restart:
 	rcu_read_lock();
@@ -917,6 +931,7 @@ static void bdi_split_work_to_wbs(struct backing_dev_info *bdi,
  */
 void cgroup_writeback_umount(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (atomic_read(&isw_nr_in_flight)) {
 		/*
 		 * Use rcu_barrier() to wait for all pending callbacks to
@@ -929,6 +944,7 @@ void cgroup_writeback_umount(void)
 
 static int __init cgroup_writeback_init(void)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	isw_wq = alloc_workqueue("inode_switch_wbs", 0, 0);
 	if (!isw_wq)
 		return -ENOMEM;
@@ -1557,6 +1573,7 @@ static long writeback_sb_inodes(struct super_block *sb,
 	long write_chunk;
 	long wrote = 0;  /* count both pages and inodes */
 
+	trace_printk("CX____ %s[%d] ---\n", __FUNCTION__, __LINE__);
 	while (!list_empty(&wb->b_io)) {
 		struct inode *inode = wb_inode(wb->b_io.prev);
 		struct bdi_writeback *tmp_wb;
@@ -1925,6 +1942,7 @@ static long wb_check_start_all(struct bdi_writeback *wb)
 		return 0;
 
 	nr_pages = get_nr_dirty_pages();
+	trace_printk("CX____ wb nr_pages=%ld\n", nr_pages);
 	if (nr_pages) {
 		struct wb_writeback_work work = {
 			.nr_pages	= wb_split_bdi_pages(wb, nr_pages),
@@ -1951,6 +1969,7 @@ static long wb_do_writeback(struct bdi_writeback *wb)
 
 	set_bit(WB_writeback_running, &wb->state);
 	while ((work = get_next_work_item(wb)) != NULL) {
+		trace_printk("CX____ wb do next work, for_forground=%d, kupdate=%d\n", work->for_background, work->for_kupdate);
 		trace_writeback_exec(wb, work);
 		wrote += wb_writeback(wb, work);
 		finish_writeback_work(wb, work);
diff --git a/include/linux/backing-dev-defs.h b/include/linux/backing-dev-defs.h
index 07e02d6df..3942ea8d3 100644
--- a/include/linux/backing-dev-defs.h
+++ b/include/linux/backing-dev-defs.h
@@ -238,6 +238,7 @@ struct wb_lock_cookie {
  */
 static inline bool wb_tryget(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		return percpu_ref_tryget(&wb->refcnt);
 	return true;
@@ -249,6 +250,7 @@ static inline bool wb_tryget(struct bdi_writeback *wb)
  */
 static inline void wb_get(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wb != &wb->bdi->wb)
 		percpu_ref_get(&wb->refcnt);
 }
@@ -259,6 +261,7 @@ static inline void wb_get(struct bdi_writeback *wb)
  */
 static inline void wb_put(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (WARN_ON_ONCE(!wb->bdi)) {
 		/*
 		 * A driver bug might cause a file to be removed before bdi was
@@ -279,6 +282,7 @@ static inline void wb_put(struct bdi_writeback *wb)
  */
 static inline bool wb_dying(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return percpu_ref_is_dying(&wb->refcnt);
 }
 
diff --git a/include/linux/backing-dev.h b/include/linux/backing-dev.h
index c28a47cbe..e95dde758 100644
--- a/include/linux/backing-dev.h
+++ b/include/linux/backing-dev.h
@@ -248,6 +248,7 @@ static inline bool inode_cgwb_enabled(struct inode *inode)
 {
 	struct backing_dev_info *bdi = inode_to_bdi(inode);
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return cgroup_subsys_on_dfl(memory_cgrp_subsys) &&
 		cgroup_subsys_on_dfl(io_cgrp_subsys) &&
 		bdi_cap_account_dirty(bdi) &&
@@ -268,6 +269,7 @@ static inline struct bdi_writeback *wb_find_current(struct backing_dev_info *bdi
 	struct cgroup_subsys_state *memcg_css;
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	memcg_css = task_css(current, memory_cgrp_id);
 	if (!memcg_css->parent)
 		return &bdi->wb;
@@ -297,6 +299,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
 {
 	struct bdi_writeback *wb;
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	rcu_read_lock();
 	wb = wb_find_current(bdi);
 	if (wb && unlikely(!wb_tryget(wb)))
@@ -322,6 +325,7 @@ wb_get_create_current(struct backing_dev_info *bdi, gfp_t gfp)
  */
 static inline bool inode_to_wb_is_valid(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return inode->i_wb;
 }
 
@@ -335,6 +339,7 @@ static inline bool inode_to_wb_is_valid(struct inode *inode)
  */
 static inline struct bdi_writeback *inode_to_wb(const struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 #ifdef CONFIG_LOCKDEP
 	WARN_ON_ONCE(debug_locks &&
 		     (!lockdep_is_held(&inode->i_lock) &&
@@ -364,6 +369,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 {
 	rcu_read_lock();
 
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * Paired with store_release in inode_switch_wb_work_fn() and
 	 * ensures that we see the new wb if we see cleared I_WB_SWITCH.
@@ -388,6 +394,7 @@ unlocked_inode_to_wb_begin(struct inode *inode, struct wb_lock_cookie *cookie)
 static inline void unlocked_inode_to_wb_end(struct inode *inode,
 					    struct wb_lock_cookie *cookie)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (unlikely(cookie->locked))
 		xa_unlock_irqrestore(&inode->i_mapping->i_pages, cookie->flags);
 
diff --git a/include/linux/blk-cgroup.h b/include/linux/blk-cgroup.h
index 6d766a19f..d25182279 100644
--- a/include/linux/blk-cgroup.h
+++ b/include/linux/blk-cgroup.h
@@ -253,6 +253,8 @@ static inline bool blk_cgroup_congested(void)
 	struct cgroup_subsys_state *css;
 	bool ret = false;
 
+	//CX____
+	//dump_stack();
 	rcu_read_lock();
 	css = kthread_blkcg();
 	if (!css)
@@ -399,6 +401,7 @@ extern void blkcg_destroy_blkgs(struct blkcg *blkcg);
  */
 static inline void blkcg_cgwb_get(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	refcount_inc(&blkcg->cgwb_refcnt);
 }
 
@@ -414,6 +417,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg)
  */
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (refcount_dec_and_test(&blkcg->cgwb_refcnt))
 		blkcg_destroy_blkgs(blkcg);
 }
@@ -424,6 +428,7 @@ static inline void blkcg_cgwb_get(struct blkcg *blkcg) { }
 
 static inline void blkcg_cgwb_put(struct blkcg *blkcg)
 {
+	trace_printk("%s[%d] no CG-WB --- !!!!!!\n", __FUNCTION__, __LINE__);
 	/* wb isn't being accounted, so trigger destruction right away */
 	blkcg_destroy_blkgs(blkcg);
 }
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index f6dfb3073..27430fee5 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -135,6 +135,8 @@ static inline void bio_issue_init(struct bio_issue *issue,
 	issue->value = ((issue->value & BIO_ISSUE_RES_MASK) |
 			(ktime_get_ns() & BIO_ISSUE_TIME_MASK) |
 			((u64)size << BIO_ISSUE_SIZE_SHIFT));
+    printk(KERN_ERR "CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
+    trace_printk("CX____ %s[%d]: issue->value=%llu\n", __FUNCTION__, __LINE__, issue->value);
 }
 
 /*
diff --git a/include/linux/writeback.h b/include/linux/writeback.h
index fdfd04e34..bba7029e0 100644
--- a/include/linux/writeback.h
+++ b/include/linux/writeback.h
@@ -203,6 +203,7 @@ void cgroup_writeback_umount(void);
  */
 static inline void inode_attach_wb(struct inode *inode, struct page *page)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (!inode->i_wb)
 		__inode_attach_wb(inode, page);
 }
@@ -215,6 +216,7 @@ static inline void inode_attach_wb(struct inode *inode, struct page *page)
  */
 static inline void inode_detach_wb(struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (inode->i_wb) {
 		WARN_ON_ONCE(!(inode->i_state & I_CLEAR));
 		wb_put(inode->i_wb);
@@ -234,6 +236,7 @@ static inline void inode_detach_wb(struct inode *inode)
 static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
 					       struct inode *inode)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	spin_lock(&inode->i_lock);
 	inode_attach_wb(inode, NULL);
 	wbc_attach_and_unlock_inode(wbc, inode);
@@ -250,6 +253,7 @@ static inline void wbc_attach_fdatawrite_inode(struct writeback_control *wbc,
  */
 static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	/*
 	 * pageout() path doesn't attach @wbc to the inode being written
 	 * out.  This is intentional as we don't want the function to block
diff --git a/include/trace/events/writeback.h b/include/trace/events/writeback.h
index 32db72c7c..748da2e1c 100644
--- a/include/trace/events/writeback.h
+++ b/include/trace/events/writeback.h
@@ -137,11 +137,13 @@ DEFINE_EVENT(writeback_dirty_inode_template, writeback_dirty_inode,
 
 static inline unsigned int __trace_wb_assign_cgroup(struct bdi_writeback *wb)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	return wb->memcg_css->cgroup->kn->id.ino;
 }
 
 static inline unsigned int __trace_wbc_assign_cgroup(struct writeback_control *wbc)
 {
+	trace_printk("%s[%d] CG-WB ---\n", __FUNCTION__, __LINE__);
 	if (wbc->wb)
 		return __trace_wb_assign_cgroup(wbc->wb);
 	else
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index 78ef274b0..e99dcd6f3 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -2663,6 +2663,7 @@ struct task_struct *cgroup_procs_write_start(char *buf, bool threadgroup)
 	if (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)
 		return ERR_PTR(-EINVAL);
 
+    trace_printk("CX____ %s[%d]: pid=%d\n", __FUNCTION__, __LINE__, pid);
 	percpu_down_write(&cgroup_threadgroup_rwsem);
 
 	rcu_read_lock();
